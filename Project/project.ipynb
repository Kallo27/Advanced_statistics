{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: koRpus.lang.en\n",
      "\n",
      "Loading required package: koRpus\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: sylly\n",
      "\n",
      "For information on available language packages for 'koRpus', run\n",
      "\n",
      "  available.koRpus.lang()\n",
      "\n",
      "and see ?install.koRpus.lang()\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘koRpus’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:tm’:\n",
      "\n",
      "    readTagged\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "library(textstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " simple example demonstration remove stopwords r order demonstrate demonstrate demonstrate run run run runner"
     ]
    }
   ],
   "source": [
    "text <- \"This is a simple example for demonstration how to remove stopwords in R in order to demonstrate or demonstrated demonstrating. run running ran runner\"\n",
    "\n",
    "corpus <- VCorpus(VectorSource(text))\n",
    "corpus <- tm_map(corpus, content_transformer(tolower))\n",
    "corpus <- tm_map(corpus, removePunctuation)\n",
    "corpus <- tm_map(corpus, removeWords, stopwords(\"en\"))\n",
    "corpus <- tm_map(corpus, stripWhitespace)\n",
    "\n",
    "# Lemmatize words using textstem\n",
    "lemmatize_text <- function(text) {\n",
    "  lemmatized <- textstem::lemmatize_words(unlist(strsplit(text, \"\\\\s+\")))\n",
    "  paste(lemmatized, collapse = \" \")\n",
    "}\n",
    "\n",
    "# Apply lemmatization to the corpus\n",
    "corpus <- tm_map(corpus, content_transformer(lemmatize_text))\n",
    "\n",
    "cat(sapply(corpus, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_multinomial_nb <- function(classes, docs, labels) {\n",
    "  # Extract unique vocabulary from documents\n",
    "  vocab <- unique(unlist(strsplit(docs, \"\\\\s+\")))\n",
    "  \n",
    "  # Count total number of documents\n",
    "  num_docs <- length(docs)\n",
    "  \n",
    "  prior <- list()\n",
    "  condprob <- list()\n",
    "  \n",
    "  for (class in classes) {\n",
    "    # Count the number of documents in each class\n",
    "    num_docs_in_class <- sum(labels == class)\n",
    "    \n",
    "    # Calculate prior probability for each class\n",
    "    prior[[class]] <- num_docs_in_class / num_docs\n",
    "    \n",
    "    # Concatenate all text in the class\n",
    "    text_class <- paste(docs[labels == class], collapse = \" \")\n",
    "    \n",
    "    for (term in vocab) {\n",
    "      # Initialize condprob for the term if it doesn't exist\n",
    "      if (is.null(condprob[[term]])) condprob[[term]] <- list()\n",
    "      \n",
    "      # Count occurrences of term in class text\n",
    "      term_count <- sum(unlist(strsplit(text_class, \"\\\\s+\")) == term)\n",
    "      \n",
    "      # Calculate conditional probability with Laplace smoothing\n",
    "      condprob[[term]][[class]] <- (term_count + 1) / (sum(sapply(vocab, function(t) sum(unlist(strsplit(text_class, \"\\\\s+\")) == t) + 1)))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(list(vocab = vocab, prior = prior, condprob = condprob))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "apply_multinomial_nb <- function(classes, vocab, prior, condprob, doc) {\n",
    "  # Extract tokens from the document that are in the vocabulary\n",
    "  tokens <- intersect(unlist(strsplit(doc, \"\\\\s+\")), vocab)\n",
    "  \n",
    "  scores <- list()\n",
    "  \n",
    "  for (class in classes) {\n",
    "    # Initialize score for each class with log of prior probability\n",
    "    scores[[class]] <- log(prior[[class]])\n",
    "    \n",
    "    for (term in tokens) {\n",
    "      # Add log of conditional probability to score\n",
    "      if (!is.null(condprob[[term]][[class]])) {\n",
    "        scores[[class]] <- scores[[class]] + log(condprob[[term]][[class]])\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Return class with the highest score\n",
    "  return(names(scores)[which.max(unlist(scores))])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"A\"\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "docs <- c(\"this is a sample document\", \"another example document\", \"sample text for classification\")\n",
    "labels <- c(\"A\", \"B\", \"A\")\n",
    "classes <- unique(labels)\n",
    "\n",
    "# Train the model\n",
    "model <- train_multinomial_nb(classes, docs, labels)\n",
    "\n",
    "# Apply the model to a new document\n",
    "new_doc <- \"this is a new document for classification\"\n",
    "predicted_class <- apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, new_doc)\n",
    "print(predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
