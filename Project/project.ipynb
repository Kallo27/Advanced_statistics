{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P08 - Multinomial Naive Bayes classifier for Fake News recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name   | ID |\n",
    "| -------- | ------- |\n",
    "| Calandra Buonaura Lorenzo | 2107761     |\n",
    "| Turci Andrea  |2106724   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an era where information dissemination is increasingly digital and instantaneous, the proliferation of fake news has become a pervasive issue. The New York Times defines fake news as \"a made-up story with an intention to deceive\", highlighting its purpose to confuse or mislead the audience. This phenomenon is predominantly propagated through social media platforms and various online applications, embedding itself deeply in our daily lives. The ability to distinguish between fake and authentic news has emerged as one of the most pressing challenges for the modern news industry.\n",
    "\n",
    "This assignment delves into the application of Multinomial Naive Bayes classifiers for the recognition of fake news. These classifiers are non-machine-learning classifier, renowned for their efficacy in text data analysis, particularly in classification tasks where the objective is to categorize text into multiple classes. Why are they called Multinomial Naive Bayes classifier?\n",
    "- The term \"multinomial\" assumes that the features (word counts) are generated from a multinomial distribution; this distribution models the number of times an event occurs in a fixed number of trials, where each trial results in one of several possible outcomes and it is particularly suitable for data where the features are counts or frequencies, such as the number of times a word appears in a document. This assumption, while simplifying computations, has been proven effective in practical text classification tasks.\n",
    "- The term \"naive\" refers to the strong (and often unrealistic) assumption of conditional independence between features. In mathematical terms, this means that the probability of observing the conjunction of features is simply the product of the probabilities for each individual feature, given the class label.\n",
    "- The term \"Bayes\" refers to the fact that these classifiers are based on Bayes' Theorem and they leverage the probability of features (words or phrases) to determine the likelihood of a particular category (e.g., fake or real news). \n",
    "\n",
    "The goal of this project is to implement a Multinomial Naive Bayes classifier in R. The focus will be on evaluating the performance of this classifier in categorizing som sort of document, coming mostly from social media. The project will involve several key steps:\n",
    "\n",
    "- Data Preprocessing: this involves cleaning the text data to remove noise, such as punctuation and stop words, and transforming the text into a format suitable for analysis, typically using techniques like tokenization and lemmatization.\n",
    "- Model Training: using the cleaned dataset, the Multinomial Naive Bayes classifier will be trained to learn the patterns associated with fake and real news. This involves calculating the prior probabilities of each class and then the conditional probabilities (posterior) of each word given the class.\n",
    "- Hyperparameter Tuning: the model's parameters will be adjusted to improve its performance.\n",
    "- Validation and Testing: the final model will be validated using cross-validation techniques to ensure its robustness and tested on unseen data to assess its generalization capability. This will help in understanding the effectiveness of the model in distinguishing fake news from real news when facing unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different datasets were used in order to test the algortim. \n",
    "The first $^1$ contains 10240 documents classified according to six labels:\n",
    "\n",
    "* $\\textit{Barely-True: 0}$\n",
    "* $\\textit{False: 1}$\n",
    "* $\\textit{Half-True: 2}$\n",
    "* $\\textit{Mostly-True: 3}$\n",
    "* $\\textit{Not-Known: 4}$\n",
    "* $\\textit{True: 5}$\n",
    "\n",
    "In contrast, the second $^2$ contains 20387 news articles, which were merged into a single dataset in which class 0 was assigned to reliable news and class 1 to unreliable news.\n",
    "\n",
    "In the following sections, the algorithm will be applied initially to the first dataset, the results of which will be displayed, and only afterwards to the second dataset.\n",
    "\n",
    "$^1 \\small{https://www.kaggle.com/datasets/anmolkumar/fake-news-content-detection/overview}$\n",
    "\n",
    "$^2 \\small{https://www.kaggle.com/competitions/fake-news/overview}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text classification, we are given a description $d \\in X$ of a document, where $X$ is the document space; and a fixed set of classes $C = {c_1, c_2, \\dots, c_J}$ (also called categories or labels). We are also given a training set $D$ of labeled documents $<d, c>$, where $d, c \\in X \\times C$.  Using a learning method or learning algorithm, we then wish to learn a classifier or classification function $\\gamma$ that maps documents to classes: $\\gamma : X \\rightarrow C$ (so we are in the supervised learning framework).\n",
    "\n",
    "The probability of a text or document $d$ to belong to category $c$ can be obtained from Bayes' theorem: \n",
    "\n",
    "$$ P(c|d) \\approx P(c) \\ \\prod_k P(t_k | c) $$\n",
    "\n",
    "where $P(t_k | c)$ corresponds to the probability that the term $t_k$ appears in a document of class $c$; this probability is a measure of how much $t_k$ contributes for $c$ to be the correct class to assign to the document. As we can see, here the \"naive\" property enters, as each word is considered independent from the others and the probability of each document is simply propportional to the product of the conditional probability of each word the document contains. The conditional probability is estimated as the relative frequency of the term $t$ in the documents belonging to the class $c$: \n",
    "\n",
    "$$ P(t|c) = \\frac{T_{ct}}{\\sum_{t'} T_{ct'}} $$\n",
    "\n",
    "where $T_{ct}$ is the number of times the term $t$ appears in the document of class $c$. Finally $P(c)$ is the a priori probability that a document belongs to class $c$; in general, the priori probability is estimated by exploiting relative frequencies: \n",
    "\n",
    "$$ P(c) = \\frac{N_c}{N} $$\n",
    "\n",
    "where $N_c$ corresponds to the total number of documents of class $c$ and $N$ is the total number of documents in the dataset.\n",
    "\n",
    "Since the probability $P(c|d)$ is given by the product of many conditional probabilities, it is computationally advantageous to consider the logarithms of these quantities and sum them together. Finally we can say that in the Naive Bayes classification the best class is the most likely or maximum a posteriori (MAP) class $c_{map}$:\n",
    "\n",
    "$$c_{map} = argmax_{c \\in C} \\; P(c|d) = argmax_{c \\in C} \\; \\left[\\log(P(c))+ \\sum_{k} \\log(P(t_k | c))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the practical implementation of a Naive Bayes classifier, it is common to encounter the zero probability problem. This occurs when a term $t_k$ does not appear in any of the documents of a class $c$. In that case, the conditional probability $P(t_k | c)$ will be zero, and consequently, the product of the conditional probabilities will become zero, canceling the total probability $P(c|d)$. To avoid this problem, a technique called \"Laplace smoothing\" or \"add-one smoothing\" is used, and the prior probability and the conditional probability are modified as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The a priori probability $P(c)$ remains the same, since it is not affected by the presence or absence of terms.\n",
    "\n",
    "$$ P(c) = \\frac{N_c}{N} $$\n",
    " \n",
    "The conditional probability, instead, is calculated by adding 1 to the numerator (number of times the term $t$ appears in class $c$ documents) and adding the total number of distinct terms $|V|$ to the denominator (total number of occurrences of all terms in class $c$ documents):\n",
    "\n",
    "$$ P(t|c) = \\frac{T_{ct} + 1}{\\sum_{t'} T_{ct'} + |V|} $$\n",
    " \n",
    "where:\n",
    "\n",
    "- $T_{ct}$ is the number of times the term $t$ appears in the documents of class $c$.\n",
    "- $\\sum_{t'} T_{ct'}$ is the total number of occurrences of all terms in the documents of class $c$.\n",
    "- $|V|$ is the number of distinct terms in the total vocabulary, so the length of the vocabulary.\n",
    "\n",
    "The final formula for determining the best class $c_{map}$ remains similar, but uses smoothed conditional probabilities:\n",
    "\n",
    "$$ c_{map} = argmax_{c \\in C} \\; P(c|d) = argmax_{c \\in C} \\; \\left[\\log(P(c))+ \\sum_{k} \\log\\left(\\frac{T_{ct_k} + 1}{\\sum_{t'} T_{ct'} + |V|}\\right)\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of Laplace smoothing has several advantages:\n",
    "- It avoids the problem of zero probabilities.\n",
    "- It provides a more robust estimate of conditional probabilities.\n",
    "- It allows better handling of new or rare words that may appear in test documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we present the most important functions that are used for the analysis of the two dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to import the libraries used for the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: NLP\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: koRpus.lang.en\n",
      "\n",
      "Loading required package: koRpus\n",
      "\n",
      "Loading required package: sylly\n",
      "\n",
      "For information on available language packages for 'koRpus', run\n",
      "\n",
      "  available.koRpus.lang()\n",
      "\n",
      "and see ?install.koRpus.lang()\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘koRpus’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:tm’:\n",
      "\n",
      "    readTagged\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(tm)\n",
    "library(textstem)\n",
    "library(SnowballC)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries `tm`, `textsetm` and `SnowballC` are used for data tokenization, lemmatization and stemming. The library `dplyr`, instead, is designed to make data manipulation easier and more intuitive (allowing for example piping operations, thanks to the pipe operator `>%>`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we need to pre-process our data in order to clean them from noise and unuseful words (like stopwords), so we proceed with a data cleaning, which includes tokenization, lemmatization and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "change_labels <- function(labels) {\n",
    "  label_map <- c(\"0\" = 2, \"1\" = 1, \"2\" = 3, \"3\" = 4, \"4\" = 0, \"5\" = 5)\n",
    "  new_labels <- sapply(labels, function(label) label_map[as.character(label)])\n",
    "  return(new_labels)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function change_labels remaps a set of categorical labels according to a specified mapping (defined by  `label_map`). The input labels is expected to be a vector of labels and then the `sapply` function is used to apply this mapping to each label in the input vector; the new vector of remapped labels is then returned by the function. This function is used only for the first dataset, in order to make the labelling more consistent with the meaning of each label; after the mapping, the correspondance is:\n",
    "\n",
    "* $\\textit{Not-Known: 0}$\n",
    "* $\\textit{False: 1}$\n",
    "* $\\textit{Barely-True: 2}$\n",
    "* $\\textit{Half-True: 3}$\n",
    "* $\\textit{Mostly-True: 4}$\n",
    "* $\\textit{True: 5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lemmatize_text <- function(text) {\n",
    "  lemmatized <- textstem::lemmatize_words(unlist(strsplit(text, \"\\\\s+\")))\n",
    "  lemmatized <- SnowballC::wordStem(lemmatized, language = \"en\")\n",
    "\n",
    "  return(paste(lemmatized, collapse = \" \"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lemmatize_text` function processes an input string text by first lemmatizing and then stemming each word. Lemmatization (using `textstem::lemmatize_words`) converts words to their base or dictionary form, while stemming (using `SnowballC::wordStem`) reduces words to their stem form. The input text is split into individual words, processed, and then recombined into a single string, which is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "filter_non_english_words <- function(text) {\n",
    "  tokens <- unlist(strsplit(text, \"\\\\s+\"))\n",
    "  is_english <- hunspell::hunspell_check(tokens)\n",
    "  english_tokens <- tokens[is_english]\n",
    "  cleaned_text <- paste(english_tokens, collapse = \" \")\n",
    "  return(cleaned_text)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "This function `filter_non_english_words` removes non-English words from a given input string text. It tokenizes the text into individual words, checks each word for being an English word using `hunspell::hunspell_check`, and retains only the words identified as English. The cleaned text, composed only of English words, is then reassembled into a single string and returned. This is done because our datasets are made mostly of english articles and the non-english words are not significative in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "remove_numbers_inside_words <- function(text) {\n",
    "  words <- unlist(strsplit(text, \"\\\\s+\"))\n",
    "\n",
    "  clean_words <- lapply(words, function(word) {\n",
    "    if (grepl(\"\\\\d\", word)) {\n",
    "      word <- gsub(\"\\\\d\", \"\", word)\n",
    "    }\n",
    "    return(word)\n",
    "  })\n",
    "\n",
    "  cleaned_text <- paste(clean_words, collapse = \" \")\n",
    "  return(cleaned_text)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `remove_numbers_inside_words` function cleans a given input string text by removing any numerical digits within words. It splits the text into individual words, processes each word to remove digits (using `gsub`), and then recombines the cleaned words into a single string. The resulting string, with numbers removed from within words, is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "clean <- function(document, tokenize = TRUE, lemmatize = TRUE) {\n",
    "  clean_doc <- tm::VCorpus(tm::VectorSource(document))\n",
    "\n",
    "  if (tokenize) {\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::content_transformer(tolower))\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::removePunctuation)\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::removeWords, tm::stopwords(\"en\"))\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::content_transformer(filter_non_english_words))\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::content_transformer(remove_numbers_inside_words))\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::stripWhitespace)\n",
    "  }\n",
    "\n",
    "  if (lemmatize) {\n",
    "    clean_doc <- tm::tm_map(clean_doc, tm::content_transformer(lemmatize_text))\n",
    "  }\n",
    "\n",
    "  return(sapply(clean_doc, NLP::content))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean` function performs comprehensive text cleaning on an input document. It first converts the input document into a text corpus using `tm::VCorpus`. If tokenize is set to `TRUE`, the function applies a series of transformations: converting text to lowercase, removing punctuation, removing stop words, filtering non-English words, removing numbers from within words, and stripping whitespace. If lemmatize is set to `TRUE`, it also lemmatizes the text. The function returns the cleaned document as a character vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "clean_empty_rows <- function(dataframe) {\n",
    "  empty_rows <- which(nchar(trimws(dataframe$Text)) == 0)\n",
    "  if (length(empty_rows) != 0) {\n",
    "    dataframe <- dataframe[-empty_rows, ]\n",
    "  }\n",
    "  return(dataframe)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_empty_rows` function removes rows from a dataframe where the `Text` column is empty or contains only whitespace. It identifies such rows trimming the whitespaces (`trimws`) and then counting how many character are still there (`nchar`), then excludes rows with zero characters from the dataframe. The cleaned dataframe, with empty rows removed, is returned. This function ensures that the dataframe only contains rows with meaningful text data, because after the cleaning process it could happen that all the words contained in a row are neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cleaning process, we need to identify the vocabulary of the clean dataset, which is a sorted list of unique words contained in the dataset. As explained later, we identified different techniques for building the vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_vocabulary_six <- function(document, threshold) {\n",
    "  words <- unlist(strsplit(document, \"\\\\s+\"))\n",
    "  words <- words[words != \"\"]\n",
    "  words_table <- table(words)\n",
    "\n",
    "  words_freq <- as.data.frame(words_table, stringsAsFactors = FALSE)\n",
    "  colnames(words_freq) <- c(\"word\", \"occurrencies\")\n",
    "\n",
    "  vocabulary <- words_freq[words_freq$occurrencies >= threshold, ]$word\n",
    "  return(vocabulary)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_vocabulary_six` function creates a vocabulary list from a given text document (of the first dataset, the one with six labels) by including only those words that occur at least a specified number of times. It begins by tokenizing the input document, splitting it into individual words using spaces as delimiters (empty strings that may result from this split are removed). The function then constructs a frequency table of these words, which is converted into a data frame with columns named word and occurrencies, representing each unique word and its frequency of occurrence, respectively. The function filters this data frame to include only those words whose frequency meets or exceeds the specified threshold. The resulting vocabulary is returned as a vector of words that meet this criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_vocabulary_tags <- function(df, threshold) {\n",
    "  tag_texts <- list()\n",
    "  all_tags <- unique(unlist(strsplit(df$Tag, \",\")))\n",
    "\n",
    "  for (tag in all_tags) {\n",
    "    matching_docs <- df[grep(tag, df$Tag), \"Text\"]\n",
    "    doc <- paste(matching_docs, collapse = \" \")\n",
    "\n",
    "    voc <- get_vocabulary_six(doc, threshold)\n",
    "    tag_texts <- append(tag_texts, voc)\n",
    "  }\n",
    "\n",
    "  return(sort(unique(unlist(tag_texts))))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_vocabulary_tags` function constructs a vocabulary list based on the tags associated with text documents in a data frame. It first identifies all unique tags in the `Tag` column by splitting the tags on commas and finding unique entries. For each unique tag, the function retrieves the texts of all documents associated with that tag, concatenating them into a single text string. It then uses the `get_vocabulary_six` function to generate a vocabulary list for this concatenated text, filtering words based on the specified threshold. The vocabularies for all tags are combined into a single list, which is returned as a sorted vector of unique words. This approach ensures that the vocabulary reflects the terms most commonly associated with each tag, based on their frequency in the relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_vocabulary_two <- function(document, threshold) {\n",
    "  words <- unlist(strsplit(document, \"\\\\s+\"))\n",
    "  words <- words[words != \"\"]\n",
    "  words_table <- table(words)\n",
    "\n",
    "  words_freq <- as.data.frame(words_table, stringsAsFactors = FALSE)\n",
    "  colnames(words_freq) <- c(\"word\", \"occurrencies\")\n",
    "\n",
    "  total_words <- sum(words_freq$occurrencies)\n",
    "  words_freq$occurrencies <- words_freq$occurrencies / total_words\n",
    "\n",
    "  vocabulary <- words_freq[words_freq$occurrencies >= threshold, ]$word\n",
    "  return(voc = vocabulary)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_vocabulary_two` function also generates a vocabulary list from a text document (from the second dataset, so two labels only), but does so based on the relative frequency of words. Similar to `get_vocabulary_six`, it starts by tokenizing the document and removing any empty strings. It creates a frequency table of the words and converts it into a data frame with word and occurrencies columns. The function then calculates the total number of words and converts the occurrencies column to represent the relative frequency of each word. Words whose relative frequency meets or exceeds the specified threshold are filtered and stored in the vocabulary, which is then returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data pre-processing and vocabulary building, we are ready for the training of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_multinomial_nb <- function(classes, data, threshold, type) {\n",
    "  n <- length(data$Text)\n",
    "\n",
    "  if (type == \"Six\") {\n",
    "    vocabulary <- get_vocabulary_six(paste(data$Text, collapse = \" \"), threshold)\n",
    "  } else if (type == \"Two\") {\n",
    "    vocabulary <- get_vocabulary_two(paste(data$Text, collapse = \" \"), threshold)\n",
    "  } else if (type == \"Tags\") {\n",
    "    vocabulary <- get_vocabulary_tags(data, threshold)\n",
    "  } else {\n",
    "    stop(\"Invalid type specified\")\n",
    "  }\n",
    "\n",
    "  prior <- numeric(length(classes))\n",
    "  names(prior) <- classes\n",
    "  post <- matrix(0, nrow = length(vocabulary), ncol = length(classes), dimnames = list(vocabulary, classes))\n",
    "\n",
    "  for (c in seq_along(classes)) {\n",
    "    class_label <- classes[c]\n",
    "    docs_in_class <- data[data$Label == class_label, \"Text\"]\n",
    "    prior[c] <- length(docs_in_class) / n\n",
    "\n",
    "    textc <- paste(docs_in_class, collapse = \" \")\n",
    "    tokens <- table(strsplit(tolower(textc), \"\\\\W+\")[[1]])\n",
    "    vocab_counts <- sapply(vocabulary, function(t) if (t %in% names(tokens)) tokens[t] else 0)\n",
    "\n",
    "    post[, c] <- (vocab_counts + 1) / (sum(vocab_counts) + length(vocabulary))\n",
    "  }\n",
    "\n",
    "  return(list(vocab = vocabulary, prior = prior, condprob = post))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_multinomial_nb` function trains a Multinomial Naive Bayes classifier based on text data and specified classes. This function performs several critical tasks, including constructing the vocabulary, calculating prior probabilities, and computing conditional probabilities for each class.\n",
    "\n",
    "The function starts by determining the length of the data, which is the number of text documents. It then decides how to build the vocabulary based on the specified type parameter:\n",
    "- If type is `\"Six\"`, it calls `get_vocabulary_six` to generate the vocabulary from the combined text of all documents. \n",
    "- If type is `\"Two\"`, it calls `get_vocabulary_two` and retrieves the vocabulary part of the returned list. \n",
    "- If type is `\"Tags\"`, it calls `get_vocabulary_tags` to generate a vocabulary based on tags associated with the documents. \n",
    "- If an invalid type is provided, the function stops and raises an error.\n",
    "\n",
    "Next, the function initializes the prior probability array and the conditional probability matrix. The prior array has a length equal to the number of classes and is named according to the class labels. The conditional probability matrix has rows corresponding to the vocabulary and columns corresponding to the classes, initialized to zeros.\n",
    "\n",
    "The function then iterates over each class to compute the prior and conditional probabilities. For each class, it filters the documents that belong to the current class and calculates the prior probability as the ratio of the number of documents in the class to the total number of documents. It concatenates the text of all documents in the class into a single string and tokenizes this string into words. It counts the frequency of each word and constructs a frequency table. The function computes the conditional probabilities using Laplace smoothing: for each word in the vocabulary, it adds one to the word count (to avoid zero probabilities) and normalizes by the total word count plus the size of the vocabulary. This ensures that every word has a non-zero probability.\n",
    "\n",
    "Finally, the function returns a list containing three elements: the vocabulary, the prior probabilities, and the conditional probability matrix. This trained model can then be used for classifying new text documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we are ready to use the trained model on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "apply_multinomial_nb <- function(classes, vocab, prior, condprob, doc) {\n",
    "  tokens <- intersect(unlist(strsplit(doc, \"\\\\s+\")), vocab)\n",
    "\n",
    "  score_matrix <- matrix(0, nrow = length(tokens), ncol = length(classes))\n",
    "  rownames(score_matrix) <- tokens\n",
    "  colnames(score_matrix) <- classes\n",
    "\n",
    "  for (c in seq_along(classes)) {\n",
    "    for (t in seq_along(tokens)) {\n",
    "      term <- tokens[t]\n",
    "      score_matrix[t, c] <- log(condprob[term, c])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  scores <- colSums(score_matrix) + log(prior)\n",
    "\n",
    "  return(names(which.max(scores)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply_multinomial_nb` function applies a trained Multinomial Naive Bayes classifier to a new document in order to classify it. This function uses the vocabulary, prior probabilities, and conditional probabilities computed during training to determine the most likely class for the given document.\n",
    "\n",
    "The function begins by tokenizing the input document into individual words. It then intersects these tokens with the provided vocabulary to ensure that only relevant words (those present both in the vocabulary and in the document) are considered. Then, a score matrix is initialized (filled with zeros), with rows representing the intersected tokens and columns representing the classes. The function then iterates over each class and each token, populating the score matrix with the log of the conditional probability of each token given the class. This involves two nested loops: the outer loop iterates over the classes, and the inner loop iterates over the tokens.\n",
    "\n",
    "Once the score matrix is populated, the function calculates the total score for each class by summing the log-probabilities in the score matrix and adding the log of the prior probability for each class. The class with the highest total score is selected as the predicted class.\n",
    "The function returns the name of the class with the maximum score, indicating the predicted classification for the input document. This approach ensures that the classification takes into account both the prior probability of each class and the likelihood of the document given each class, making use of the Naive Bayes assumption that the presence of each word is conditionally independent given the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined two function to perform hyperparameters tuning, leveraging the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "validation <- function(dataset, thresholds, type) {\n",
    "  seventy_percent <- floor(length(dataset$Text) * 0.7)\n",
    "  eightyfive_percent <- floor(length(dataset$Text) * 0.85)\n",
    "  n <- nrow(dataset)\n",
    "\n",
    "  dataset <- dataset[sample(n), ]\n",
    "\n",
    "  training_set <- dataset[1:seventy_percent, ]\n",
    "  validation_set <- dataset[(seventy_percent + 1):eightyfive_percent, ]\n",
    "\n",
    "  accuracies <- numeric(length(thresholds))\n",
    "  classes <- as.integer(sort(unique(dataset$Label)))\n",
    "\n",
    "  for (i in seq_along(thresholds)) {\n",
    "    model <- train_multinomial_nb(classes, training_set, thresholds[[i]], type)\n",
    "    pred_labels <- sapply(validation_set$Text, function(doc) {\n",
    "      apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "    })\n",
    "\n",
    "    correct_predictions <- sum(validation_set$Label == pred_labels)\n",
    "    total_predictions <- length(validation_set$Label)\n",
    "    accuracies[[i]] <- correct_predictions / total_predictions\n",
    "  }\n",
    "\n",
    "  return(data.frame(threshold = thresholds, accuracy = accuracies))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `validation` function is designed to validate a Multinomial Naive Bayes classifier over different threshold values. It does this by splitting the dataset into training and validation sets; then, for each threshold, it trains the model on the training set, and then evaluates its performance on the validation set. The function then returns a data frame containing the accuracy for each threshold value tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "kfold_cross_validation <- function(dataset, k = 5, thresholds, type) {\n",
    "  n <- nrow(dataset)\n",
    "  fold_size <- floor(n / k)\n",
    "\n",
    "  accuracies <- matrix(0, nrow = k, ncol = length(thresholds))\n",
    "  classes <- as.integer(sort(unique(dataset$Label)))\n",
    "\n",
    "  for (fold in 1:k) {\n",
    "    validation_indices <- ((fold - 1) * fold_size + 1):(fold * fold_size)\n",
    "    train_indices <- setdiff(1:n, validation_indices)\n",
    "    training_set <- dataset[train_indices, ]\n",
    "    validation_set <- dataset[validation_indices, ]\n",
    "\n",
    "    for (i in seq_along(thresholds)) {\n",
    "      model <- train_multinomial_nb(classes, training_set, thresholds[i], type)\n",
    "\n",
    "      pred_labels <- sapply(validation_set$Text, function(doc) {\n",
    "        apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "      })\n",
    "\n",
    "      correct_predictions <- sum(validation_set$Label == pred_labels)\n",
    "      total_predictions <- length(validation_set$Label)\n",
    "      accuracies[fold, i] <- correct_predictions / total_predictions\n",
    "    }\n",
    "  }\n",
    "\n",
    "  mean_accuracies <- colMeans(accuracies)\n",
    "  return(data.frame(threshold = thresholds, mean_accuracy = mean_accuracies))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kfold_cross_validation` function performs k-fold cross-validation on a given dataset to evaluate the performance of a Multinomial Naive Bayes classifier with different threshold values. This function helps to assess the classifier's accuracy by splitting the data into k subsets (folds) and iteratively training and validating the model on these folds.\n",
    "\n",
    "The function starts by determining the number of rows (documents) in the dataset and calculating the size of each fold. It initializes a matrix accuracies to store the accuracy results for each fold and each threshold value. The unique class labels in the dataset are sorted and stored as integers in the classes vector.\n",
    "\n",
    "The function then enters a loop that iterates over each fol: for each fold, it determines the indices of the validation set and the training set, which consists of all documents not in the validation set. Within each fold, the function iterates over the specified threshold values: for each threshold, it trains a Multinomial Naive Bayes classifier using the `train_multinomial_nb` function, which builds a vocabulary, computes prior probabilities, and calculates conditional probabilities based on the training set. \n",
    "\n",
    "Next, the function applies the trained model to each document in the validation set using the `apply_multinomial_nb` function, which classifies the document based on the trained model and returns the predicted class label. The function compares the predicted labels to the actual labels of the validation set to count the number of correct predictions: the accuracy for each threshold and fold is computed as the ratio of correct predictions to the total number of predictions and stored in the accuracies matrix.\n",
    "\n",
    "After completing the cross-validation process for all folds and thresholds, the function calculates the mean accuracy for each threshold by taking the column-wise mean of the accuracies matrix. The function returns a data frame containing the threshold values and their corresponding mean accuracies.\n",
    "\n",
    "This cross-validation approach ensures a robust evaluation of the classifier's performance by training and validating the model on different subsets of the data, thereby reducing the risk of overfitting and providing a more reliable estimate of the classifier's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Six-label dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously introduced, the first dataset we analyze is composed of documents with assigned one of six labels, which indicate the level of truthness of each document, and a tag that indicates the main topics of the document. We upload the data as a dataframe using the `read.csv()` function, naming the three columns. First of all, as previously explained, we change the labels in order to make their meaning consistent with their value. Secondly, we save the unique labels and tags in two vectors, which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Label</th><th scope=col>Text</th><th scope=col>Tag</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1</td><td>Says the Annies List political group supports third-trimester abortions on demand.                                                                         </td><td>abortion                          </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>3</td><td>When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.              </td><td>energy,history,job-accomplishments</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>4</td><td>Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"                                                  </td><td>foreign-policy                    </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1</td><td>Health care reform legislation is likely to mandate free sex change surgeries.                                                                             </td><td>health-care                       </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>3</td><td>The economic turnaround started at the end of my term.                                                                                                     </td><td>economy,jobs                      </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>5</td><td>The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.</td><td>education                         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & Label & Text & Tag\\\\\n",
       "  & <dbl> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & 1 & Says the Annies List political group supports third-trimester abortions on demand.                                                                          & abortion                          \\\\\n",
       "\t2 & 3 & When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.               & energy,history,job-accomplishments\\\\\n",
       "\t3 & 4 & Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"                                                   & foreign-policy                    \\\\\n",
       "\t4 & 1 & Health care reform legislation is likely to mandate free sex change surgeries.                                                                              & health-care                       \\\\\n",
       "\t5 & 3 & The economic turnaround started at the end of my term.                                                                                                      & economy,jobs                      \\\\\n",
       "\t6 & 5 & The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades. & education                         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 3\n",
       "\n",
       "| <!--/--> | Label &lt;dbl&gt; | Text &lt;chr&gt; | Tag &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | 1 | Says the Annies List political group supports third-trimester abortions on demand.                                                                          | abortion                           |\n",
       "| 2 | 3 | When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.               | energy,history,job-accomplishments |\n",
       "| 3 | 4 | Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"                                                   | foreign-policy                     |\n",
       "| 4 | 1 | Health care reform legislation is likely to mandate free sex change surgeries.                                                                              | health-care                        |\n",
       "| 5 | 3 | The economic turnaround started at the end of my term.                                                                                                      | economy,jobs                       |\n",
       "| 6 | 5 | The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades. | education                          |\n",
       "\n"
      ],
      "text/plain": [
       "  Label\n",
       "1 1    \n",
       "2 3    \n",
       "3 4    \n",
       "4 1    \n",
       "5 3    \n",
       "6 5    \n",
       "  Text                                                                                                                                                       \n",
       "1 Says the Annies List political group supports third-trimester abortions on demand.                                                                         \n",
       "2 When did the decline of coal start? It started when natural gas took off that started to begin in (President George W.) Bushs administration.              \n",
       "3 Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"                                                  \n",
       "4 Health care reform legislation is likely to mandate free sex change surgeries.                                                                             \n",
       "5 The economic turnaround started at the end of my term.                                                                                                     \n",
       "6 The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.\n",
       "  Tag                               \n",
       "1 abortion                          \n",
       "2 energy,history,job-accomplishments\n",
       "3 foreign-policy                    \n",
       "4 health-care                       \n",
       "5 economy,jobs                      \n",
       "6 education                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset <- read.csv(\"six_label_dataset.csv\", col.names = c(\"Label\", \"Text\", \"Tag\"))\n",
    "dataset$Label <- change_labels(dataset$Label)\n",
    "head(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0</li><li>1</li><li>2</li><li>3</li><li>4</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 1\n",
       "3. 2\n",
       "4. 3\n",
       "5. 4\n",
       "6. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 1 2 3 4 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes <- as.integer(sort(unique(dataset$Label)))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'10-news-tampa-bay'</li><li>'abc-news-week'</li><li>'abortion'</li><li>'afghanistan'</li><li>'after-the-fact'</li><li>'agriculture'</li><li>'Alcohol'</li><li>'animals'</li><li>'autism'</li><li>'bankruptcy'</li><li>'baseball'</li><li>'bipartisanship'</li><li>'bush-administration'</li><li>'campaign-advertising'</li><li>'campaign-finance'</li><li>'candidates-biography'</li><li>'cap-and-trade'</li><li>'census'</li><li>'children'</li><li>'china'</li><li>'city-budget'</li><li>'city-government'</li><li>'civil-rights'</li><li>'climate-change'</li><li>'colbert-report'</li><li>'congress'</li><li>'congressional-rules'</li><li>'consumer-safety'</li><li>'corporations'</li><li>'corrections-and-updates'</li><li>'county-budget'</li><li>'county-government'</li><li>'crime'</li><li>'criminal-justice'</li><li>'death-penalty'</li><li>'debates'</li><li>'debt'</li><li>'deficit'</li><li>'disability'</li><li>'diversity'</li><li>'drugs'</li><li>'ebola'</li><li>'economy'</li><li>'education'</li><li>'elections'</li><li>'energy'</li><li>'environment'</li><li>'ethics'</li><li>'fake-news'</li><li>'families'</li><li>'federal-budget'</li><li>'financial-regulation'</li><li>'fires'</li><li>'florida'</li><li>'florida-amendments'</li><li>'food'</li><li>'food-safety'</li><li>'foreign-policy'</li><li>'gambling'</li><li>'gas-prices'</li><li>'gays-and-lesbians'</li><li>'government-efficiency'</li><li>'government-regulation'</li><li>'guns'</li><li>'health-care'</li><li>'history'</li><li>'homeland-security'</li><li>'homeless'</li><li>'housing'</li><li>'human-rights'</li><li>'hunger'</li><li>'immigration'</li><li>'income'</li><li>'infrastructure'</li><li>'iraq'</li><li>'islam'</li><li>'israel'</li><li>'job-accomplishments'</li><li>'jobs'</li><li>'kagan-nomination'</li><li>'labor'</li><li>'legal-issues'</li><li>'lottery'</li><li>'marijuana'</li><li>'market-regulation'</li><li>'marriage'</li><li>'medicaid'</li><li>'medicare'</li><li>'message-machine'</li><li>'message-machine-2012'</li><li>'message-machine-2014'</li><li>'military'</li><li>'natural-disasters'</li><li>'new-hampshire-2012'</li><li>'nuclear'</li><li>'obama-birth-certificate'</li><li>'occupy-wall-street'</li><li>'oil-spill'</li><li>'patriotism'</li><li>'pensions'</li><li>'polls'</li><li>'pop-culture'</li><li>'population'</li><li>'poverty'</li><li>'privacy'</li><li>'public-health'</li><li>'public-safety'</li><li>'public-service'</li><li>'pundits'</li><li>'recreation'</li><li>'redistricting'</li><li>'religion'</li><li>'retirement'</li><li>'science'</li><li>'sexuality'</li><li>'small-business'</li><li>'social-security'</li><li>'sotomayor-nomination'</li><li>'space'</li><li>'sports'</li><li>'state-budget'</li><li>'state-finances'</li><li>'states'</li><li>'stimulus'</li><li>'supreme-court'</li><li>'taxes'</li><li>'technology'</li><li>'terrorism'</li><li>'tourism'</li><li>'trade'</li><li>'transparency'</li><li>'transportation'</li><li>'unions'</li><li>'urban'</li><li>'veterans'</li><li>'voting-record'</li><li>'water'</li><li>'wealth'</li><li>'weather'</li><li>'welfare'</li><li>'women'</li><li>'workers'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '10-news-tampa-bay'\n",
       "\\item 'abc-news-week'\n",
       "\\item 'abortion'\n",
       "\\item 'afghanistan'\n",
       "\\item 'after-the-fact'\n",
       "\\item 'agriculture'\n",
       "\\item 'Alcohol'\n",
       "\\item 'animals'\n",
       "\\item 'autism'\n",
       "\\item 'bankruptcy'\n",
       "\\item 'baseball'\n",
       "\\item 'bipartisanship'\n",
       "\\item 'bush-administration'\n",
       "\\item 'campaign-advertising'\n",
       "\\item 'campaign-finance'\n",
       "\\item 'candidates-biography'\n",
       "\\item 'cap-and-trade'\n",
       "\\item 'census'\n",
       "\\item 'children'\n",
       "\\item 'china'\n",
       "\\item 'city-budget'\n",
       "\\item 'city-government'\n",
       "\\item 'civil-rights'\n",
       "\\item 'climate-change'\n",
       "\\item 'colbert-report'\n",
       "\\item 'congress'\n",
       "\\item 'congressional-rules'\n",
       "\\item 'consumer-safety'\n",
       "\\item 'corporations'\n",
       "\\item 'corrections-and-updates'\n",
       "\\item 'county-budget'\n",
       "\\item 'county-government'\n",
       "\\item 'crime'\n",
       "\\item 'criminal-justice'\n",
       "\\item 'death-penalty'\n",
       "\\item 'debates'\n",
       "\\item 'debt'\n",
       "\\item 'deficit'\n",
       "\\item 'disability'\n",
       "\\item 'diversity'\n",
       "\\item 'drugs'\n",
       "\\item 'ebola'\n",
       "\\item 'economy'\n",
       "\\item 'education'\n",
       "\\item 'elections'\n",
       "\\item 'energy'\n",
       "\\item 'environment'\n",
       "\\item 'ethics'\n",
       "\\item 'fake-news'\n",
       "\\item 'families'\n",
       "\\item 'federal-budget'\n",
       "\\item 'financial-regulation'\n",
       "\\item 'fires'\n",
       "\\item 'florida'\n",
       "\\item 'florida-amendments'\n",
       "\\item 'food'\n",
       "\\item 'food-safety'\n",
       "\\item 'foreign-policy'\n",
       "\\item 'gambling'\n",
       "\\item 'gas-prices'\n",
       "\\item 'gays-and-lesbians'\n",
       "\\item 'government-efficiency'\n",
       "\\item 'government-regulation'\n",
       "\\item 'guns'\n",
       "\\item 'health-care'\n",
       "\\item 'history'\n",
       "\\item 'homeland-security'\n",
       "\\item 'homeless'\n",
       "\\item 'housing'\n",
       "\\item 'human-rights'\n",
       "\\item 'hunger'\n",
       "\\item 'immigration'\n",
       "\\item 'income'\n",
       "\\item 'infrastructure'\n",
       "\\item 'iraq'\n",
       "\\item 'islam'\n",
       "\\item 'israel'\n",
       "\\item 'job-accomplishments'\n",
       "\\item 'jobs'\n",
       "\\item 'kagan-nomination'\n",
       "\\item 'labor'\n",
       "\\item 'legal-issues'\n",
       "\\item 'lottery'\n",
       "\\item 'marijuana'\n",
       "\\item 'market-regulation'\n",
       "\\item 'marriage'\n",
       "\\item 'medicaid'\n",
       "\\item 'medicare'\n",
       "\\item 'message-machine'\n",
       "\\item 'message-machine-2012'\n",
       "\\item 'message-machine-2014'\n",
       "\\item 'military'\n",
       "\\item 'natural-disasters'\n",
       "\\item 'new-hampshire-2012'\n",
       "\\item 'nuclear'\n",
       "\\item 'obama-birth-certificate'\n",
       "\\item 'occupy-wall-street'\n",
       "\\item 'oil-spill'\n",
       "\\item 'patriotism'\n",
       "\\item 'pensions'\n",
       "\\item 'polls'\n",
       "\\item 'pop-culture'\n",
       "\\item 'population'\n",
       "\\item 'poverty'\n",
       "\\item 'privacy'\n",
       "\\item 'public-health'\n",
       "\\item 'public-safety'\n",
       "\\item 'public-service'\n",
       "\\item 'pundits'\n",
       "\\item 'recreation'\n",
       "\\item 'redistricting'\n",
       "\\item 'religion'\n",
       "\\item 'retirement'\n",
       "\\item 'science'\n",
       "\\item 'sexuality'\n",
       "\\item 'small-business'\n",
       "\\item 'social-security'\n",
       "\\item 'sotomayor-nomination'\n",
       "\\item 'space'\n",
       "\\item 'sports'\n",
       "\\item 'state-budget'\n",
       "\\item 'state-finances'\n",
       "\\item 'states'\n",
       "\\item 'stimulus'\n",
       "\\item 'supreme-court'\n",
       "\\item 'taxes'\n",
       "\\item 'technology'\n",
       "\\item 'terrorism'\n",
       "\\item 'tourism'\n",
       "\\item 'trade'\n",
       "\\item 'transparency'\n",
       "\\item 'transportation'\n",
       "\\item 'unions'\n",
       "\\item 'urban'\n",
       "\\item 'veterans'\n",
       "\\item 'voting-record'\n",
       "\\item 'water'\n",
       "\\item 'wealth'\n",
       "\\item 'weather'\n",
       "\\item 'welfare'\n",
       "\\item 'women'\n",
       "\\item 'workers'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '10-news-tampa-bay'\n",
       "2. 'abc-news-week'\n",
       "3. 'abortion'\n",
       "4. 'afghanistan'\n",
       "5. 'after-the-fact'\n",
       "6. 'agriculture'\n",
       "7. 'Alcohol'\n",
       "8. 'animals'\n",
       "9. 'autism'\n",
       "10. 'bankruptcy'\n",
       "11. 'baseball'\n",
       "12. 'bipartisanship'\n",
       "13. 'bush-administration'\n",
       "14. 'campaign-advertising'\n",
       "15. 'campaign-finance'\n",
       "16. 'candidates-biography'\n",
       "17. 'cap-and-trade'\n",
       "18. 'census'\n",
       "19. 'children'\n",
       "20. 'china'\n",
       "21. 'city-budget'\n",
       "22. 'city-government'\n",
       "23. 'civil-rights'\n",
       "24. 'climate-change'\n",
       "25. 'colbert-report'\n",
       "26. 'congress'\n",
       "27. 'congressional-rules'\n",
       "28. 'consumer-safety'\n",
       "29. 'corporations'\n",
       "30. 'corrections-and-updates'\n",
       "31. 'county-budget'\n",
       "32. 'county-government'\n",
       "33. 'crime'\n",
       "34. 'criminal-justice'\n",
       "35. 'death-penalty'\n",
       "36. 'debates'\n",
       "37. 'debt'\n",
       "38. 'deficit'\n",
       "39. 'disability'\n",
       "40. 'diversity'\n",
       "41. 'drugs'\n",
       "42. 'ebola'\n",
       "43. 'economy'\n",
       "44. 'education'\n",
       "45. 'elections'\n",
       "46. 'energy'\n",
       "47. 'environment'\n",
       "48. 'ethics'\n",
       "49. 'fake-news'\n",
       "50. 'families'\n",
       "51. 'federal-budget'\n",
       "52. 'financial-regulation'\n",
       "53. 'fires'\n",
       "54. 'florida'\n",
       "55. 'florida-amendments'\n",
       "56. 'food'\n",
       "57. 'food-safety'\n",
       "58. 'foreign-policy'\n",
       "59. 'gambling'\n",
       "60. 'gas-prices'\n",
       "61. 'gays-and-lesbians'\n",
       "62. 'government-efficiency'\n",
       "63. 'government-regulation'\n",
       "64. 'guns'\n",
       "65. 'health-care'\n",
       "66. 'history'\n",
       "67. 'homeland-security'\n",
       "68. 'homeless'\n",
       "69. 'housing'\n",
       "70. 'human-rights'\n",
       "71. 'hunger'\n",
       "72. 'immigration'\n",
       "73. 'income'\n",
       "74. 'infrastructure'\n",
       "75. 'iraq'\n",
       "76. 'islam'\n",
       "77. 'israel'\n",
       "78. 'job-accomplishments'\n",
       "79. 'jobs'\n",
       "80. 'kagan-nomination'\n",
       "81. 'labor'\n",
       "82. 'legal-issues'\n",
       "83. 'lottery'\n",
       "84. 'marijuana'\n",
       "85. 'market-regulation'\n",
       "86. 'marriage'\n",
       "87. 'medicaid'\n",
       "88. 'medicare'\n",
       "89. 'message-machine'\n",
       "90. 'message-machine-2012'\n",
       "91. 'message-machine-2014'\n",
       "92. 'military'\n",
       "93. 'natural-disasters'\n",
       "94. 'new-hampshire-2012'\n",
       "95. 'nuclear'\n",
       "96. 'obama-birth-certificate'\n",
       "97. 'occupy-wall-street'\n",
       "98. 'oil-spill'\n",
       "99. 'patriotism'\n",
       "100. 'pensions'\n",
       "101. 'polls'\n",
       "102. 'pop-culture'\n",
       "103. 'population'\n",
       "104. 'poverty'\n",
       "105. 'privacy'\n",
       "106. 'public-health'\n",
       "107. 'public-safety'\n",
       "108. 'public-service'\n",
       "109. 'pundits'\n",
       "110. 'recreation'\n",
       "111. 'redistricting'\n",
       "112. 'religion'\n",
       "113. 'retirement'\n",
       "114. 'science'\n",
       "115. 'sexuality'\n",
       "116. 'small-business'\n",
       "117. 'social-security'\n",
       "118. 'sotomayor-nomination'\n",
       "119. 'space'\n",
       "120. 'sports'\n",
       "121. 'state-budget'\n",
       "122. 'state-finances'\n",
       "123. 'states'\n",
       "124. 'stimulus'\n",
       "125. 'supreme-court'\n",
       "126. 'taxes'\n",
       "127. 'technology'\n",
       "128. 'terrorism'\n",
       "129. 'tourism'\n",
       "130. 'trade'\n",
       "131. 'transparency'\n",
       "132. 'transportation'\n",
       "133. 'unions'\n",
       "134. 'urban'\n",
       "135. 'veterans'\n",
       "136. 'voting-record'\n",
       "137. 'water'\n",
       "138. 'wealth'\n",
       "139. 'weather'\n",
       "140. 'welfare'\n",
       "141. 'women'\n",
       "142. 'workers'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] \"10-news-tampa-bay\"       \"abc-news-week\"          \n",
       "  [3] \"abortion\"                \"afghanistan\"            \n",
       "  [5] \"after-the-fact\"          \"agriculture\"            \n",
       "  [7] \"Alcohol\"                 \"animals\"                \n",
       "  [9] \"autism\"                  \"bankruptcy\"             \n",
       " [11] \"baseball\"                \"bipartisanship\"         \n",
       " [13] \"bush-administration\"     \"campaign-advertising\"   \n",
       " [15] \"campaign-finance\"        \"candidates-biography\"   \n",
       " [17] \"cap-and-trade\"           \"census\"                 \n",
       " [19] \"children\"                \"china\"                  \n",
       " [21] \"city-budget\"             \"city-government\"        \n",
       " [23] \"civil-rights\"            \"climate-change\"         \n",
       " [25] \"colbert-report\"          \"congress\"               \n",
       " [27] \"congressional-rules\"     \"consumer-safety\"        \n",
       " [29] \"corporations\"            \"corrections-and-updates\"\n",
       " [31] \"county-budget\"           \"county-government\"      \n",
       " [33] \"crime\"                   \"criminal-justice\"       \n",
       " [35] \"death-penalty\"           \"debates\"                \n",
       " [37] \"debt\"                    \"deficit\"                \n",
       " [39] \"disability\"              \"diversity\"              \n",
       " [41] \"drugs\"                   \"ebola\"                  \n",
       " [43] \"economy\"                 \"education\"              \n",
       " [45] \"elections\"               \"energy\"                 \n",
       " [47] \"environment\"             \"ethics\"                 \n",
       " [49] \"fake-news\"               \"families\"               \n",
       " [51] \"federal-budget\"          \"financial-regulation\"   \n",
       " [53] \"fires\"                   \"florida\"                \n",
       " [55] \"florida-amendments\"      \"food\"                   \n",
       " [57] \"food-safety\"             \"foreign-policy\"         \n",
       " [59] \"gambling\"                \"gas-prices\"             \n",
       " [61] \"gays-and-lesbians\"       \"government-efficiency\"  \n",
       " [63] \"government-regulation\"   \"guns\"                   \n",
       " [65] \"health-care\"             \"history\"                \n",
       " [67] \"homeland-security\"       \"homeless\"               \n",
       " [69] \"housing\"                 \"human-rights\"           \n",
       " [71] \"hunger\"                  \"immigration\"            \n",
       " [73] \"income\"                  \"infrastructure\"         \n",
       " [75] \"iraq\"                    \"islam\"                  \n",
       " [77] \"israel\"                  \"job-accomplishments\"    \n",
       " [79] \"jobs\"                    \"kagan-nomination\"       \n",
       " [81] \"labor\"                   \"legal-issues\"           \n",
       " [83] \"lottery\"                 \"marijuana\"              \n",
       " [85] \"market-regulation\"       \"marriage\"               \n",
       " [87] \"medicaid\"                \"medicare\"               \n",
       " [89] \"message-machine\"         \"message-machine-2012\"   \n",
       " [91] \"message-machine-2014\"    \"military\"               \n",
       " [93] \"natural-disasters\"       \"new-hampshire-2012\"     \n",
       " [95] \"nuclear\"                 \"obama-birth-certificate\"\n",
       " [97] \"occupy-wall-street\"      \"oil-spill\"              \n",
       " [99] \"patriotism\"              \"pensions\"               \n",
       "[101] \"polls\"                   \"pop-culture\"            \n",
       "[103] \"population\"              \"poverty\"                \n",
       "[105] \"privacy\"                 \"public-health\"          \n",
       "[107] \"public-safety\"           \"public-service\"         \n",
       "[109] \"pundits\"                 \"recreation\"             \n",
       "[111] \"redistricting\"           \"religion\"               \n",
       "[113] \"retirement\"              \"science\"                \n",
       "[115] \"sexuality\"               \"small-business\"         \n",
       "[117] \"social-security\"         \"sotomayor-nomination\"   \n",
       "[119] \"space\"                   \"sports\"                 \n",
       "[121] \"state-budget\"            \"state-finances\"         \n",
       "[123] \"states\"                  \"stimulus\"               \n",
       "[125] \"supreme-court\"           \"taxes\"                  \n",
       "[127] \"technology\"              \"terrorism\"              \n",
       "[129] \"tourism\"                 \"trade\"                  \n",
       "[131] \"transparency\"            \"transportation\"         \n",
       "[133] \"unions\"                  \"urban\"                  \n",
       "[135] \"veterans\"                \"voting-record\"          \n",
       "[137] \"water\"                   \"wealth\"                 \n",
       "[139] \"weather\"                 \"welfare\"                \n",
       "[141] \"women\"                   \"workers\"                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args <- sort(unique(unlist(strsplit(dataset$Tag, \",\"))))\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an initial look to the dataset, we can see how many unique words the dataset contains before cleaning it. Then, after applying the `clean()` function and performing lemmatization and stemming, we can see how much the vocabulary has been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "21678"
      ],
      "text/latex": [
       "21678"
      ],
      "text/markdown": [
       "21678"
      ],
      "text/plain": [
       "[1] 21678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc <- length(get_vocabulary_six(dataset$Text, threshold = 1))\n",
    "len_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset$Text <- clean(dataset$Text)\n",
    "dataset <- clean_empty_rows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "5142"
      ],
      "text/latex": [
       "5142"
      ],
      "text/markdown": [
       "5142"
      ],
      "text/plain": [
       "[1] 5142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc_cleaned <- length(get_vocabulary_six(dataset$Text, threshold = 1))\n",
    "len_voc_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2074"
      ],
      "text/latex": [
       "2074"
      ],
      "text/markdown": [
       "2074"
      ],
      "text/plain": [
       "[1] 2074"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc_cleaned <- length(get_vocabulary_six(dataset$Text, threshold = 5))\n",
    "len_voc_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cleaning process reduces a lot the total number of words that are actually unique in our dataset; in particular we get that, using the previously presented techniques for stemming and lemmatizing, the final vocabulary is only 23.7% of the initial vocabulary. If we include also a frequency check, choosing a threshold greater than 1, we are able to reduce the dimension of the vocabulary even more; for example, for `threshold = 5`, the final vocabulary is only 9.6% of the initial vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the preprocessing of the dataset, we are ready to train our Multinomial Naive Bayes model; the first thing to do is to divide the whole dataset in training set, validation set and test set, in order to tune the hyper-parameter of the model annd study its accuracy on unseen data. Before the division we randomly permutate the dataset, in order to remove possible correlation between consecutive documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "seventy_percent <- floor(length(dataset$Text) * 0.7)\n",
    "eightyfive_percent <- floor(length(dataset$Text) * 0.85)\n",
    "n <- nrow(dataset)\n",
    "\n",
    "dataset <- dataset[sample(n), ]\n",
    "\n",
    "training_set <- dataset[1:seventy_percent, ]\n",
    "validation_set <- dataset[(seventy_percent + 1):eightyfive_percent, ]\n",
    "test_set <- dataset[(eightyfive_percent + 1):n, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we consider `threshold = 3` as an example; later in the notebook we proceed to a tuning of this parameter using the validation set and then choosing the model that has the best accuracy on it. After the training, the output of the model are presented to give an idea of how things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model <- train_multinomial_nb(classes, training_set, threshold = 3, type = \"Six\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [1] \"2\"             \"3\"             \"4\"             \"5\"            \n",
      "   [5] \"6\"             \"abil\"          \"abl\"           \"abolish\"      \n",
      "   [9] \"abort\"         \"absente\"       \"absolut\"       \"abus\"         \n",
      "  [13] \"academi\"       \"acceler\"       \"accept\"        \"access\"       \n",
      "  [17] \"accid\"         \"accident\"      \"accommod\"      \"accord\"       \n",
      "  [21] \"account\"       \"accumul\"       \"accus\"         \"achiev\"       \n",
      "  [25] \"acknowledg\"    \"acorn\"         \"acr\"           \"across\"       \n",
      "  [29] \"act\"           \"action\"        \"activ\"         \"activist\"     \n",
      "  [33] \"actual\"        \"ad\"            \"add\"           \"addict\"       \n",
      "  [37] \"addit\"         \"address\"       \"adjust\"        \"administr\"    \n",
      "  [41] \"admir\"         \"admiss\"        \"admit\"         \"adopt\"        \n",
      "  [45] \"adult\"         \"advanc\"        \"advantag\"      \"advertis\"     \n",
      "  [49] \"advis\"         \"advisor\"       \"advisori\"      \"advoc\"        \n",
      "  [53] \"advocaci\"      \"affair\"        \"affect\"        \"affili\"       \n",
      "  [57] \"afford\"        \"age\"           \"agenc\"         \"agendum\"      \n",
      "  [61] \"agent\"         \"aggress\"       \"ago\"           \"agre\"         \n",
      "  [65] \"agreement\"     \"agricultur\"    \"ahead\"         \"aid\"          \n",
      "  [69] \"aim\"           \"air\"           \"aircraft\"      \"airport\"      \n",
      "  [73] \"akin\"          \"alcohol\"       \"alien\"         \"alleg\"        \n",
      "  [77] \"alli\"          \"allow\"         \"almost\"        \"alon\"         \n",
      "  [81] \"along\"         \"alreadi\"       \"also\"          \"alter\"        \n",
      "  [85] \"altern\"        \"although\"      \"alway\"         \"amaz\"         \n",
      "  [89] \"ambassador\"    \"amend\"         \"ammunit\"       \"amnesti\"      \n",
      "  [93] \"among\"         \"amount\"        \"analysi\"       \"analyst\"      \n",
      "  [97] \"anchor\"        \"angl\"          \"angri\"         \"anim\"         \n",
      " [101] \"announc\"       \"annual\"        \"anoth\"         \"answer\"       \n",
      " [105] \"ant\"           \"anybodi\"       \"anymor\"        \"anyon\"        \n",
      " [109] \"anyth\"         \"anytim\"        \"anywher\"       \"apolog\"       \n",
      " [113] \"appar\"         \"appeal\"        \"appear\"        \"appl\"         \n",
      " [117] \"appli\"         \"applic\"        \"appoint\"       \"appointe\"     \n",
      " [121] \"apprehend\"     \"approach\"      \"appropri\"      \"approv\"       \n",
      " [125] \"approxim\"      \"area\"          \"arena\"         \"argu\"         \n",
      " [129] \"arm\"           \"armi\"          \"armor\"         \"around\"       \n",
      " [133] \"arrest\"        \"art\"           \"ask\"           \"aspect\"       \n",
      " [137] \"assassin\"      \"assault\"       \"assembl\"       \"assess\"       \n",
      " [141] \"asset\"         \"assist\"        \"associ\"        \"assur\"        \n",
      " [145] \"asthma\"        \"attack\"        \"attempt\"       \"attend\"       \n",
      " [149] \"attent\"        \"attitud\"       \"attorney\"      \"attract\"      \n",
      " [153] \"attribut\"      \"audienc\"       \"audit\"         \"august\"       \n",
      " [157] \"author\"        \"auto\"          \"automat\"       \"automobil\"    \n",
      " [161] \"avail\"         \"averag\"        \"avoid\"         \"award\"        \n",
      " [165] \"away\"          \"babi\"          \"back\"          \"background\"   \n",
      " [169] \"backlog\"       \"bad\"           \"bag\"           \"bail\"         \n",
      " [173] \"bailey\"        \"bailout\"       \"baker\"         \"balanc\"       \n",
      " [177] \"balloon\"       \"ballot\"        \"ban\"           \"bank\"         \n",
      " [181] \"bankrupt\"      \"bankruptci\"    \"bar\"           \"bare\"         \n",
      " [185] \"bargain\"       \"barrel\"        \"barrow\"        \"base\"         \n",
      " [189] \"basebal\"       \"basi\"          \"basic\"         \"bathroom\"     \n",
      " [193] \"battl\"         \"battlefield\"   \"bay\"           \"beach\"        \n",
      " [197] \"bear\"          \"beat\"          \"becom\"         \"bed\"          \n",
      " [201] \"beef\"          \"beer\"          \"beg\"           \"begin\"        \n",
      " [205] \"behavior\"      \"behind\"        \"belief\"        \"believ\"       \n",
      " [209] \"bell\"          \"belong\"        \"bench\"         \"beneficiari\"  \n",
      " [213] \"benefit\"       \"beyond\"        \"bibl\"          \"bicycl\"       \n",
      " [217] \"bid\"           \"big\"           \"bike\"          \"bill\"         \n",
      " [221] \"billion\"       \"billionair\"    \"bin\"           \"bipartisan\"   \n",
      " [225] \"birth\"         \"birther\"       \"bishop\"        \"bite\"         \n",
      " [229] \"black\"         \"blame\"         \"blind\"         \"block\"        \n",
      " [233] \"blow\"          \"blue\"          \"blunt\"         \"board\"        \n",
      " [237] \"bob\"           \"bodi\"          \"bomb\"          \"bond\"         \n",
      " [241] \"bonus\"         \"book\"          \"boost\"         \"boot\"         \n",
      " [245] \"border\"        \"borrow\"        \"boss\"          \"bottom\"       \n",
      " [249] \"bowl\"          \"boxer\"         \"boy\"           \"bracket\"      \n",
      " [253] \"brag\"          \"brain\"         \"branch\"        \"brand\"        \n",
      " [257] \"brat\"          \"brave\"         \"break\"         \"breast\"       \n",
      " [261] \"bridg\"         \"bring\"         \"brother\"       \"brotherhood\"  \n",
      " [265] \"brown\"         \"buck\"          \"budg\"          \"budget\"       \n",
      " [269] \"build\"         \"bulb\"          \"bullet\"        \"bunch\"        \n",
      " [273] \"burden\"        \"bureau\"        \"bureaucraci\"   \"bureaucrat\"   \n",
      " [277] \"burn\"          \"burr\"          \"bus\"           \"bush\"         \n",
      " [281] \"busi\"          \"buy\"           \"buyer\"         \"cabinet\"      \n",
      " [285] \"calcul\"        \"call\"          \"camera\"        \"camp\"         \n",
      " [289] \"campaign\"      \"campus\"        \"can\"           \"cancel\"       \n",
      " [293] \"cancer\"        \"candid\"        \"candidaci\"     \"cant\"         \n",
      " [297] \"cantor\"        \"cap\"           \"capit\"         \"capitol\"      \n",
      " [301] \"captur\"        \"car\"           \"carbon\"        \"card\"         \n",
      " [305] \"care\"          \"career\"        \"carri\"         \"cartel\"       \n",
      " [309] \"carter\"        \"case\"          \"cash\"          \"casino\"       \n",
      " [313] \"cast\"          \"casualti\"      \"cat\"           \"catch\"        \n",
      " [317] \"cathol\"        \"caucus\"        \"caus\"          \"ceil\"         \n",
      " [321] \"celebr\"        \"cell\"          \"census\"        \"cent\"         \n",
      " [325] \"center\"        \"central\"       \"centuri\"       \"ceremoni\"     \n",
      " [329] \"certain\"       \"certif\"        \"chair\"         \"chairman\"     \n",
      " [333] \"challeng\"      \"chamber\"       \"champion\"      \"championship\" \n",
      " [337] \"chanc\"         \"chang\"         \"charg\"         \"charit\"       \n",
      " [341] \"charli\"        \"charter\"       \"cheap\"         \"cheat\"        \n",
      " [345] \"check\"         \"chees\"         \"chemic\"        \"chief\"        \n",
      " [349] \"child\"         \"childhood\"     \"china\"         \"choic\"        \n",
      " [353] \"choos\"         \"christian\"     \"chuck\"         \"church\"       \n",
      " [357] \"cigarett\"      \"circumst\"      \"citat\"         \"cite\"         \n",
      " [361] \"citi\"          \"citizen\"       \"citizenship\"   \"civil\"        \n",
      " [365] \"civilian\"      \"claim\"         \"class\"         \"classifi\"     \n",
      " [369] \"classroom\"     \"clean\"         \"clear\"         \"cliff\"        \n",
      " [373] \"climat\"        \"climb\"         \"clinic\"        \"close\"        \n",
      " [377] \"cloth\"         \"club\"          \"clunker\"       \"coach\"        \n",
      " [381] \"coal\"          \"coalit\"        \"coast\"         \"cocain\"       \n",
      " [385] \"code\"          \"cold\"          \"collaps\"       \"colleagu\"     \n",
      " [389] \"collect\"       \"colleg\"        \"color\"         \"combat\"       \n",
      " [393] \"combin\"        \"come\"          \"command\"       \"comment\"      \n",
      " [397] \"commerc\"       \"commerci\"      \"commiss\"       \"commission\"   \n",
      " [401] \"commit\"        \"committe\"      \"common\"        \"commonwealth\" \n",
      " [405] \"communist\"     \"communiti\"     \"commut\"        \"compact\"      \n",
      " [409] \"compani\"       \"compar\"        \"compens\"       \"competit\"     \n",
      " [413] \"complet\"       \"compli\"        \"comprehens\"    \"compris\"      \n",
      " [417] \"compromis\"     \"comput\"        \"conceal\"       \"concentr\"     \n",
      " [421] \"concern\"       \"conclud\"       \"condemn\"       \"condit\"       \n",
      " [425] \"conduct\"       \"confeder\"      \"confer\"        \"confirm\"      \n",
      " [429] \"confisc\"       \"conflict\"      \"congest\"       \"congress\"     \n",
      " [433] \"congression\"   \"congressman\"   \"connect\"       \"consecut\"     \n",
      " [437] \"consent\"       \"conserv\"       \"consid\"        \"consider\"     \n",
      " [441] \"consist\"       \"constant\"      \"constitu\"      \"constitut\"    \n",
      " [445] \"construct\"     \"consult\"       \"consum\"        \"consumpt\"     \n",
      " [449] \"contain\"       \"continu\"       \"contracept\"    \"contract\"     \n",
      " [453] \"contractor\"    \"contribut\"     \"contributor\"   \"control\"      \n",
      " [457] \"controversi\"   \"convent\"       \"convert\"       \"convict\"      \n",
      " [461] \"cook\"          \"cooper\"        \"cop\"           \"core\"         \n",
      " [465] \"corpor\"        \"correct\"       \"corridor\"      \"corrupt\"      \n",
      " [469] \"cosponsor\"     \"cost\"          \"cotton\"        \"council\"      \n",
      " [473] \"counsel\"       \"count\"         \"counterpart\"   \"counti\"       \n",
      " [477] \"countri\"       \"coupl\"         \"cours\"         \"court\"        \n",
      " [481] \"cover\"         \"coverag\"       \"cowboy\"        \"crack\"        \n",
      " [485] \"crackdown\"     \"crash\"         \"creat\"         \"creation\"     \n",
      " [489] \"creator\"       \"credit\"        \"crime\"         \"crimin\"       \n",
      " [493] \"crippl\"        \"crisi\"         \"criterion\"     \"critic\"       \n",
      " [497] \"cross\"         \"crowd\"         \"cruis\"         \"cure\"         \n",
      " [501] \"currenc\"       \"current\"       \"custom\"        \"cut\"          \n",
      " [505] \"cycl\"          \"czar\"          \"dad\"           \"daili\"        \n",
      " [509] \"dairi\"         \"damag\"         \"danger\"        \"databas\"      \n",
      " [513] \"date\"          \"datum\"         \"daughter\"      \"day\"          \n",
      " [517] \"dc\"            \"dead\"          \"deal\"          \"dealer\"       \n",
      " [521] \"dean\"          \"death\"         \"debat\"         \"debt\"         \n",
      " [525] \"debunk\"        \"decad\"         \"decid\"         \"decis\"        \n",
      " [529] \"declar\"        \"declin\"        \"decreas\"       \"dedic\"        \n",
      " [533] \"deduct\"        \"deep\"          \"deer\"          \"default\"      \n",
      " [537] \"defeat\"        \"defend\"        \"defens\"        \"deficit\"      \n",
      " [541] \"defin\"         \"definit\"       \"degre\"         \"delay\"        \n",
      " [545] \"deleg\"         \"delet\"         \"deliv\"         \"demand\"       \n",
      " [549] \"democraci\"     \"democrat\"      \"deni\"          \"dental\"       \n",
      " [553] \"depart\"        \"depend\"        \"deploy\"        \"deport\"       \n",
      " [557] \"deposit\"       \"depress\"       \"depriv\"        \"deputi\"       \n",
      " [561] \"deriv\"         \"describ\"       \"deserv\"        \"design\"       \n",
      " [565] \"despit\"        \"destroy\"       \"detail\"        \"detain\"       \n",
      " [569] \"detaine\"       \"determin\"      \"devast\"        \"develop\"      \n",
      " [573] \"devic\"         \"dictat\"        \"die\"           \"differ\"       \n",
      " [577] \"difficult\"     \"dime\"          \"dinner\"        \"direct\"       \n",
      " [581] \"director\"      \"dirti\"         \"disabl\"        \"disagre\"      \n",
      " [585] \"disappear\"     \"disast\"        \"disclos\"       \"disclosur\"    \n",
      " [589] \"discov\"        \"discretionari\" \"discrimin\"     \"discuss\"      \n",
      " [593] \"diseas\"        \"disgrac\"       \"dismantl\"      \"dispar\"       \n",
      " [597] \"dispos\"        \"disproportion\" \"disput\"        \"disrupt\"      \n",
      " [601] \"distribut\"     \"district\"      \"divers\"        \"divis\"        \n",
      " [605] \"divorc\"        \"do\"            \"doctor\"        \"document\"     \n",
      " [609] \"dog\"           \"dollar\"        \"domain\"        \"dome\"         \n",
      " [613] \"domest\"        \"donat\"         \"donor\"         \"door\"         \n",
      " [617] \"doubl\"         \"doubt\"         \"downgrad\"      \"dozen\"        \n",
      " [621] \"draft\"         \"drain\"         \"dramat\"        \"drastic\"      \n",
      " [625] \"draw\"          \"dream\"         \"drill\"         \"drink\"        \n",
      " [629] \"drive\"         \"driver\"        \"drone\"         \"drop\"         \n",
      " [633] \"dropout\"       \"drug\"          \"duck\"          \"due\"          \n",
      " [637] \"dump\"          \"duti\"          \"earli\"         \"earmark\"      \n",
      " [641] \"earn\"          \"earner\"        \"earth\"         \"eas\"          \n",
      " [645] \"easi\"          \"east\"          \"eat\"           \"econom\"       \n",
      " [649] \"economi\"       \"economist\"     \"ed\"            \"educ\"         \n",
      " [653] \"effect\"        \"effici\"        \"effort\"        \"eight\"        \n",
      " [657] \"eighti\"        \"either\"        \"elder\"         \"elect\"        \n",
      " [661] \"elector\"       \"electr\"        \"electron\"      \"elementari\"   \n",
      " [665] \"elig\"          \"elimin\"        \"els\"           \"elsewher\"     \n",
      " [669] \"email\"         \"embassi\"       \"embrac\"        \"emerg\"        \n",
      " [673] \"emin\"          \"emiss\"         \"employ\"        \"employe\"      \n",
      " [677] \"enact\"         \"encourag\"      \"end\"           \"endors\"       \n",
      " [681] \"enemi\"         \"energi\"        \"enforc\"        \"engag\"        \n",
      " [685] \"engin\"         \"enjoy\"         \"enorm\"         \"enough\"       \n",
      " [689] \"enrol\"         \"ensur\"         \"enter\"         \"enterpris\"    \n",
      " [693] \"entir\"         \"entitl\"        \"environ\"       \"environment\"  \n",
      " [697] \"equal\"         \"equip\"         \"equival\"       \"escap\"        \n",
      " [701] \"especi\"        \"essenti\"       \"establish\"     \"estat\"        \n",
      " [705] \"estim\"         \"etc\"           \"ethanol\"       \"ethic\"        \n",
      " [709] \"even\"          \"event\"         \"eventu\"        \"ever\"         \n",
      " [713] \"everi\"         \"everybodi\"     \"everyon\"       \"everyth\"      \n",
      " [717] \"evid\"          \"exact\"         \"exam\"          \"exampl\"       \n",
      " [721] \"exceed\"        \"except\"        \"exchang\"       \"exclud\"       \n",
      " [725] \"exclus\"        \"execut\"        \"exempt\"        \"exercis\"      \n",
      " [729] \"exist\"         \"expand\"        \"expans\"        \"expect\"       \n",
      " [733] \"expens\"        \"experi\"        \"expert\"        \"expir\"        \n",
      " [737] \"explain\"       \"explod\"        \"exploit\"       \"explor\"       \n",
      " [741] \"explos\"        \"export\"        \"expressli\"     \"extend\"       \n",
      " [745] \"extens\"        \"extort\"        \"extra\"         \"extrem\"       \n",
      " [749] \"extremist\"     \"eye\"           \"f\"             \"face\"         \n",
      " [753] \"facil\"         \"fact\"          \"factor\"        \"factori\"      \n",
      " [757] \"faculti\"       \"fail\"          \"failur\"        \"fair\"         \n",
      " [761] \"faith\"         \"fall\"          \"fals\"          \"famili\"       \n",
      " [765] \"fanci\"         \"far\"           \"fare\"          \"farm\"         \n",
      " [769] \"farmer\"        \"fast\"          \"fatal\"         \"father\"       \n",
      " [773] \"favor\"         \"favorit\"       \"featur\"        \"feder\"        \n",
      " [777] \"fee\"           \"feed\"          \"feel\"          \"fellow\"       \n",
      " [781] \"felon\"         \"feloni\"        \"femal\"         \"fenc\"         \n",
      " [785] \"fertil\"        \"few\"           \"field\"         \"fifteen\"      \n",
      " [789] \"fifti\"         \"fight\"         \"fighter\"       \"figur\"        \n",
      " [793] \"file\"          \"filibust\"      \"fill\"          \"film\"         \n",
      " [797] \"final\"         \"financ\"        \"financi\"       \"find\"         \n",
      " [801] \"fine\"          \"fingerprint\"   \"finish\"        \"fire\"         \n",
      " [805] \"firearm\"       \"firefight\"     \"firm\"          \"first\"        \n",
      " [809] \"fiscal\"        \"fish\"          \"fisher\"        \"fit\"          \n",
      " [813] \"five\"          \"fix\"           \"fl\"            \"flag\"         \n",
      " [817] \"flat\"          \"flee\"          \"fleet\"         \"fli\"          \n",
      " [821] \"flight\"        \"flint\"         \"flood\"         \"floor\"        \n",
      " [825] \"flow\"          \"focus\"         \"folk\"          \"follow\"       \n",
      " [829] \"food\"          \"fool\"          \"foot\"          \"footbal\"      \n",
      " [833] \"forbid\"        \"forc\"          \"ford\"          \"foreclosur\"   \n",
      " [837] \"foreign\"       \"forest\"        \"forfeitur\"     \"form\"         \n",
      " [841] \"former\"        \"fort\"          \"forti\"         \"fortun\"       \n",
      " [845] \"forward\"       \"foster\"        \"found\"         \"foundat\"      \n",
      " [849] \"founder\"       \"four\"          \"fox\"           \"frack\"        \n",
      " [853] \"frame\"         \"franchis\"      \"frank\"         \"fraud\"        \n",
      " [857] \"fraudul\"       \"free\"          \"freedom\"       \"freez\"        \n",
      " [861] \"fresh\"         \"friend\"        \"front\"         \"fuel\"         \n",
      " [865] \"full\"          \"fulli\"         \"function\"      \"fund\"         \n",
      " [869] \"fundrais\"      \"funnel\"        \"futur\"         \"gain\"         \n",
      " [873] \"gallon\"        \"gambl\"         \"gambler\"       \"game\"         \n",
      " [877] \"gang\"          \"gap\"           \"gas\"           \"gasolin\"      \n",
      " [881] \"gay\"           \"gen\"           \"general\"       \"generat\"      \n",
      " [885] \"generous\"      \"genet\"         \"get\"           \"giant\"        \n",
      " [889] \"gift\"          \"girl\"          \"give\"          \"glad\"         \n",
      " [893] \"glass\"         \"global\"        \"gm\"            \"go\"           \n",
      " [897] \"goal\"          \"god\"           \"gold\"          \"golf\"         \n",
      " [901] \"good\"          \"googl\"         \"gov\"           \"govern\"       \n",
      " [905] \"governor\"      \"governorship\"  \"grade\"         \"graduat\"      \n",
      " [909] \"graham\"        \"grand\"         \"granit\"        \"grant\"        \n",
      " [913] \"great\"         \"green\"         \"greenhous\"     \"grind\"        \n",
      " [917] \"groceri\"       \"gross\"         \"group\"         \"grow\"         \n",
      " [921] \"growth\"        \"guarante\"      \"guard\"         \"gubernatori\"  \n",
      " [925] \"guess\"         \"gulf\"          \"gun\"           \"gunfir\"       \n",
      " [929] \"gut\"           \"guy\"           \"gym\"           \"habit\"        \n",
      " [933] \"half\"          \"halfway\"       \"hall\"          \"ham\"          \n",
      " [937] \"hand\"          \"handgun\"       \"handl\"         \"happen\"       \n",
      " [941] \"happi\"         \"harass\"        \"harbor\"        \"hard\"         \n",
      " [945] \"hardcor\"       \"hardwork\"      \"harm\"          \"harri\"        \n",
      " [949] \"hate\"          \"have\"          \"head\"          \"headquart\"    \n",
      " [953] \"health\"        \"healthcar\"     \"healthi\"       \"hear\"         \n",
      " [957] \"heart\"         \"heavi\"         \"heavili\"       \"heck\"         \n",
      " [961] \"hedg\"          \"helicopt\"      \"hell\"          \"help\"         \n",
      " [965] \"heroin\"        \"hes\"           \"hide\"          \"high\"         \n",
      " [969] \"highway\"       \"hike\"          \"hire\"          \"histor\"       \n",
      " [973] \"histori\"       \"hit\"           \"hold\"          \"holder\"       \n",
      " [977] \"hole\"          \"holiday\"       \"home\"          \"homeland\"     \n",
      " [981] \"homeless\"      \"homeown\"       \"homicid\"       \"homosexu\"     \n",
      " [985] \"honor\"         \"hood\"          \"hook\"          \"hope\"         \n",
      " [989] \"horribl\"       \"hors\"          \"hospit\"        \"host\"         \n",
      " [993] \"hostag\"        \"hot\"           \"hour\"          \"hous\"         \n",
      " [997] \"household\"     \"huge\"          \"human\"         \"hundr\"        \n",
      "[1001] \"hunt\"          \"hunter\"        \"hurrican\"      \"hurt\"         \n",
      "[1005] \"husband\"       \"ice\"           \"id\"            \"idea\"         \n",
      "[1009] \"ident\"         \"identif\"       \"identifi\"      \"ignor\"        \n",
      "[1013] \"ii\"            \"ill\"           \"illeg\"         \"imag\"         \n",
      "[1017] \"immedi\"        \"immigr\"        \"immun\"         \"impact\"       \n",
      "[1021] \"impeach\"       \"implement\"     \"import\"        \"impos\"        \n",
      "[1025] \"imposs\"        \"imprison\"      \"improv\"        \"inact\"        \n",
      "[1029] \"incarcer\"      \"incent\"        \"incest\"        \"inch\"         \n",
      "[1033] \"incid\"         \"includ\"        \"incom\"         \"increas\"      \n",
      "[1037] \"incred\"        \"incumb\"        \"inde\"          \"independ\"     \n",
      "[1041] \"indic\"         \"individu\"      \"industri\"      \"ineffect\"     \n",
      "[1045] \"inequ\"         \"infant\"        \"inflat\"        \"influenc\"     \n",
      "[1049] \"inform\"        \"infrastructur\" \"inherit\"       \"initi\"        \n",
      "[1053] \"injur\"         \"injuri\"        \"inmat\"         \"innov\"        \n",
      "[1057] \"input\"         \"insecur\"       \"insid\"         \"inspect\"      \n",
      "[1061] \"inspir\"        \"instal\"        \"instanc\"       \"instat\"       \n",
      "[1065] \"instead\"       \"institut\"      \"instruct\"      \"insur\"        \n",
      "[1069] \"intellig\"      \"intend\"        \"intent\"        \"interest\"     \n",
      "[1073] \"interior\"      \"intern\"        \"internet\"      \"interview\"    \n",
      "[1077] \"introduc\"      \"invad\"         \"invas\"         \"invent\"       \n",
      "[1081] \"invest\"        \"investig\"      \"investor\"      \"invit\"        \n",
      "[1085] \"involv\"        \"iron\"          \"island\"        \"issu\"         \n",
      "[1089] \"item\"          \"jack\"          \"jail\"          \"japan\"        \n",
      "[1093] \"jersey\"        \"jet\"           \"jimmi\"         \"job\"          \n",
      "[1097] \"jobless\"       \"john\"          \"johnni\"        \"join\"         \n",
      "[1101] \"joint\"         \"jolli\"         \"josh\"          \"jr\"           \n",
      "[1105] \"judg\"          \"judgment\"      \"judici\"        \"jump\"         \n",
      "[1109] \"junket\"        \"just\"          \"justic\"        \"justifi\"      \n",
      "[1113] \"keep\"          \"ken\"           \"key\"           \"keyston\"      \n",
      "[1117] \"kick\"          \"kicker\"        \"kid\"           \"kidnap\"       \n",
      "[1121] \"kill\"          \"killer\"        \"kind\"          \"kindergarten\" \n",
      "[1125] \"king\"          \"kit\"           \"know\"          \"knowledg\"     \n",
      "[1129] \"lab\"           \"label\"         \"labor\"         \"lack\"         \n",
      "[1133] \"lade\"          \"ladi\"          \"lake\"          \"land\"         \n",
      "[1137] \"landslid\"      \"lane\"          \"languag\"       \"larg\"         \n",
      "[1141] \"last\"          \"late\"          \"launch\"        \"law\"          \n",
      "[1145] \"lawmak\"        \"lawsuit\"       \"lawyer\"        \"lay\"          \n",
      "[1149] \"layoff\"        \"lead\"          \"leader\"        \"leadership\"   \n",
      "[1153] \"leagu\"         \"leak\"          \"learn\"         \"leas\"         \n",
      "[1157] \"leav\"          \"lee\"           \"legal\"         \"legisl\"       \n",
      "[1161] \"legislatur\"    \"legitim\"       \"lender\"        \"lesbian\"      \n",
      "[1165] \"less\"          \"let\"           \"letter\"        \"level\"        \n",
      "[1169] \"liabil\"        \"liber\"         \"libertarian\"   \"librari\"      \n",
      "[1173] \"licens\"        \"lie\"           \"lieuten\"       \"life\"         \n",
      "[1177] \"lifetim\"       \"lift\"          \"light\"         \"like\"         \n",
      "[1181] \"limit\"         \"line\"          \"link\"          \"list\"         \n",
      "[1185] \"liter\"         \"littl\"         \"live\"          \"load\"         \n",
      "[1189] \"loan\"          \"lobbi\"         \"lobbyist\"      \"local\"        \n",
      "[1193] \"locat\"         \"lock\"          \"long\"          \"look\"         \n",
      "[1197] \"loom\"          \"loophol\"       \"lose\"          \"loss\"         \n",
      "[1201] \"lot\"           \"lotteri\"       \"love\"          \"low\"          \n",
      "[1205] \"lunch\"         \"luxuri\"        \"lynch\"         \"mac\"          \n",
      "[1209] \"machin\"        \"magazin\"       \"mail\"          \"main\"         \n",
      "[1213] \"maintain\"      \"major\"         \"make\"          \"male\"         \n",
      "[1217] \"mammogram\"     \"man\"           \"manag\"         \"mandat\"       \n",
      "[1221] \"mandatori\"     \"mani\"          \"manipul\"       \"mansion\"      \n",
      "[1225] \"manufactur\"    \"march\"         \"margin\"        \"marijuana\"    \n",
      "[1229] \"marin\"         \"mark\"          \"market\"        \"marketplac\"   \n",
      "[1233] \"marri\"         \"marriag\"       \"martial\"       \"martin\"       \n",
      "[1237] \"mass\"          \"massag\"        \"massiv\"        \"master\"       \n",
      "[1241] \"match\"         \"mate\"          \"materi\"        \"matern\"       \n",
      "[1245] \"math\"          \"mathemat\"      \"matter\"        \"max\"          \n",
      "[1249] \"maximum\"       \"may\"           \"mayb\"          \"mayor\"        \n",
      "[1253] \"meal\"          \"mean\"          \"measl\"         \"measur\"       \n",
      "[1257] \"meat\"          \"mechan\"        \"medal\"         \"median\"       \n",
      "[1261] \"medic\"         \"medicaid\"      \"medicar\"       \"medicin\"      \n",
      "[1265] \"medium\"        \"meek\"          \"meet\"          \"meltdown\"     \n",
      "[1269] \"member\"        \"memo\"          \"memori\"        \"mental\"       \n",
      "[1273] \"mention\"       \"mercuri\"       \"merger\"        \"merit\"        \n",
      "[1277] \"method\"        \"metro\"         \"metropolitan\"  \"mid\"          \n",
      "[1281] \"middl\"         \"midnight\"      \"mike\"          \"mile\"         \n",
      "[1285] \"militari\"      \"milk\"          \"mill\"          \"million\"      \n",
      "[1289] \"millionair\"    \"mind\"          \"mine\"          \"minimum\"      \n",
      "[1293] \"minist\"        \"minor\"         \"minut\"         \"mismanag\"     \n",
      "[1297] \"miss\"          \"missil\"        \"mission\"       \"mistak\"       \n",
      "[1301] \"mitt\"          \"mix\"           \"mode\"          \"model\"        \n",
      "[1305] \"moder\"         \"modern\"        \"modifi\"        \"mom\"          \n",
      "[1309] \"moment\"        \"money\"         \"month\"         \"moon\"         \n",
      "[1313] \"moratorium\"    \"mortal\"        \"mortgag\"       \"mosqu\"        \n",
      "[1317] \"mosquito\"      \"most\"          \"mother\"        \"motor\"        \n",
      "[1321] \"motorist\"      \"move\"          \"movement\"      \"movi\"         \n",
      "[1325] \"much\"          \"multipl\"       \"municip\"       \"murder\"       \n",
      "[1329] \"museum\"        \"music\"         \"must\"          \"name\"         \n",
      "[1333] \"narrow\"        \"nation\"        \"nationwid\"     \"nativ\"        \n",
      "[1337] \"natur\"         \"navi\"          \"nd\"            \"near\"         \n",
      "[1341] \"need\"          \"negat\"         \"negoti\"        \"neighbor\"     \n",
      "[1345] \"neighborhood\"  \"nelson\"        \"net\"           \"network\"      \n",
      "[1349] \"never\"         \"new\"           \"newli\"         \"news\"         \n",
      "[1353] \"newspap\"       \"newt\"          \"next\"          \"night\"        \n",
      "[1357] \"nine\"          \"nineti\"        \"nobodi\"        \"nomin\"        \n",
      "[1361] \"nomine\"        \"none\"          \"nonpartisan\"   \"nonprofit\"    \n",
      "[1365] \"north\"         \"northeast\"     \"northern\"      \"note\"         \n",
      "[1369] \"noth\"          \"now\"           \"nowher\"        \"nuclear\"      \n",
      "[1373] \"numb\"          \"number\"        \"numer\"         \"nurs\"         \n",
      "[1377] \"obes\"          \"object\"        \"obsolet\"       \"obtain\"       \n",
      "[1381] \"occas\"         \"occup\"         \"occupi\"        \"occur\"        \n",
      "[1385] \"ocean\"         \"odd\"           \"offend\"        \"offens\"       \n",
      "[1389] \"offer\"         \"offic\"         \"offici\"        \"offset\"       \n",
      "[1393] \"offshor\"       \"often\"         \"oil\"           \"old\"          \n",
      "[1397] \"one\"           \"onlin\"         \"onto\"          \"open\"         \n",
      "[1401] \"oper\"          \"opinion\"       \"oppon\"         \"opportun\"     \n",
      "[1405] \"oppos\"         \"opposit\"       \"opt\"           \"option\"       \n",
      "[1409] \"orang\"         \"order\"         \"ordin\"         \"ore\"          \n",
      "[1413] \"organ\"         \"orient\"        \"origin\"        \"other\"        \n",
      "[1417] \"outlaw\"        \"outsid\"        \"outsourc\"      \"overal\"       \n",
      "[1421] \"overdos\"       \"overhaul\"      \"overhead\"      \"overse\"       \n",
      "[1425] \"oversea\"       \"oversight\"     \"overstay\"      \"overturn\"     \n",
      "[1429] \"overwhelm\"     \"owe\"           \"own\"           \"owner\"        \n",
      "[1433] \"ownership\"     \"pace\"          \"pack\"          \"packag\"       \n",
      "[1437] \"page\"          \"pain\"          \"panel\"         \"panhandl\"     \n",
      "[1441] \"pant\"          \"paper\"         \"paperwork\"     \"parent\"       \n",
      "[1445] \"parenthood\"    \"park\"          \"parol\"         \"part\"         \n",
      "[1449] \"parti\"         \"particip\"      \"particular\"    \"partisan\"     \n",
      "[1453] \"partner\"       \"partnership\"   \"pass\"          \"passag\"       \n",
      "[1457] \"past\"          \"pastor\"        \"pat\"           \"path\"         \n",
      "[1461] \"patient\"       \"patriot\"       \"patrol\"        \"pave\"         \n",
      "[1465] \"pay\"           \"paycheck\"      \"payday\"        \"payment\"      \n",
      "[1469] \"payrol\"        \"peac\"          \"peak\"          \"peanut\"       \n",
      "[1473] \"penalti\"       \"pend\"          \"penni\"         \"pension\"      \n",
      "[1477] \"pentagon\"      \"peopl\"         \"per\"           \"percent\"      \n",
      "[1481] \"percentag\"     \"perfect\"       \"perform\"       \"period\"       \n",
      "[1485] \"perman\"        \"permiss\"       \"permit\"        \"person\"       \n",
      "[1489] \"personnel\"     \"pet\"           \"peter\"         \"pharmacist\"   \n",
      "[1493] \"phoenix\"       \"phone\"         \"phoni\"         \"photo\"        \n",
      "[1497] \"photograph\"    \"physic\"        \"physician\"     \"pick\"         \n",
      "[1501] \"pictur\"        \"piec\"          \"pilot\"         \"pink\"         \n",
      "[1505] \"pipe\"          \"pipelin\"       \"pizza\"         \"place\"        \n",
      "[1509] \"plan\"          \"plane\"         \"planet\"        \"plant\"        \n",
      "[1513] \"platform\"      \"play\"          \"player\"        \"pledg\"        \n",
      "[1517] \"plot\"          \"plummet\"       \"plus\"          \"pm\"           \n",
      "[1521] \"pocket\"        \"point\"         \"poison\"        \"pole\"         \n",
      "[1525] \"polic\"         \"polici\"        \"polit\"         \"politician\"   \n",
      "[1529] \"poll\"          \"pollut\"        \"poni\"          \"pool\"         \n",
      "[1533] \"poor\"          \"pope\"          \"popul\"         \"popular\"      \n",
      "[1537] \"pork\"          \"pornographi\"   \"port\"          \"portion\"      \n",
      "[1541] \"pose\"          \"posit\"         \"possess\"       \"possibl\"      \n",
      "[1545] \"post\"          \"postal\"        \"pot\"           \"potenti\"      \n",
      "[1549] \"pound\"         \"pour\"          \"poverti\"       \"power\"        \n",
      "[1553] \"practic\"       \"prais\"         \"pray\"          \"prayer\"       \n",
      "[1557] \"preced\"        \"precinct\"      \"predat\"        \"predatori\"    \n",
      "[1561] \"predecessor\"   \"predict\"       \"preexist\"      \"prefer\"       \n",
      "[1565] \"pregnanc\"      \"pregnant\"      \"prematur\"      \"premium\"      \n",
      "[1569] \"prepar\"        \"preschool\"     \"prescrib\"      \"prescript\"    \n",
      "[1573] \"present\"       \"presid\"        \"presidenti\"    \"press\"        \n",
      "[1577] \"pressur\"       \"pretti\"        \"prevent\"       \"previous\"     \n",
      "[1581] \"price\"         \"primari\"       \"prime\"         \"princip\"      \n",
      "[1585] \"print\"         \"prior\"         \"prioriti\"      \"prison\"       \n",
      "[1589] \"privat\"        \"privileg\"      \"probabl\"       \"probe\"        \n",
      "[1593] \"problem\"       \"procedur\"      \"process\"       \"produc\"       \n",
      "[1597] \"product\"       \"profession\"    \"profici\"       \"profit\"       \n",
      "[1601] \"program\"       \"progress\"      \"prohibit\"      \"project\"      \n",
      "[1605] \"promis\"        \"promot\"        \"proof\"         \"proper\"       \n",
      "[1609] \"properti\"      \"proport\"       \"propos\"        \"proposit\"     \n",
      "[1613] \"prosecut\"      \"prosecutor\"    \"prosper\"       \"prostat\"      \n",
      "[1617] \"prostitut\"     \"protect\"       \"protest\"       \"proud\"        \n",
      "[1621] \"prove\"         \"provid\"        \"provis\"        \"public\"       \n",
      "[1625] \"pull\"          \"pump\"          \"punish\"        \"pupil\"        \n",
      "[1629] \"purchas\"       \"purg\"          \"purpos\"        \"pursu\"        \n",
      "[1633] \"push\"          \"put\"           \"quadrupl\"      \"qualifi\"      \n",
      "[1637] \"qualiti\"       \"quarter\"       \"question\"      \"quick\"        \n",
      "[1641] \"quit\"          \"quot\"          \"race\"          \"racial\"       \n",
      "[1645] \"racist\"        \"rack\"          \"radic\"         \"radio\"        \n",
      "[1649] \"raid\"          \"rail\"          \"raini\"         \"rais\"         \n",
      "[1653] \"ralli\"         \"ram\"           \"ranch\"         \"rand\"         \n",
      "[1657] \"rank\"          \"rape\"          \"rapist\"        \"rare\"         \n",
      "[1661] \"rate\"          \"rather\"        \"ratio\"         \"ration\"       \n",
      "[1665] \"rd\"            \"reach\"         \"read\"          \"readi\"        \n",
      "[1669] \"real\"          \"realiti\"       \"realli\"        \"reason\"       \n",
      "[1673] \"rebat\"         \"rebuild\"       \"recal\"         \"receiv\"       \n",
      "[1677] \"recent\"        \"recess\"        \"recidiv\"       \"recipi\"       \n",
      "[1681] \"recogn\"        \"recommend\"     \"record\"        \"recov\"        \n",
      "[1685] \"recoveri\"      \"recruit\"       \"red\"           \"redistrict\"   \n",
      "[1689] \"reduc\"         \"reduct\"        \"reelect\"       \"refer\"        \n",
      "[1693] \"referendum\"    \"refin\"         \"reflect\"       \"reform\"       \n",
      "[1697] \"refuge\"        \"refund\"        \"refus\"         \"regard\"       \n",
      "[1701] \"regardless\"    \"regim\"         \"region\"        \"regist\"       \n",
      "[1705] \"registr\"       \"registri\"      \"regul\"         \"regular\"      \n",
      "[1709] \"regulatori\"    \"reimburs\"      \"rein\"          \"reject\"       \n",
      "[1713] \"relat\"         \"relationship\"  \"releas\"        \"reli\"         \n",
      "[1717] \"relief\"        \"religi\"        \"religion\"      \"reloc\"        \n",
      "[1721] \"remain\"        \"remark\"        \"rememb\"        \"remodel\"      \n",
      "[1725] \"remov\"         \"rend\"          \"renegoti\"      \"renew\"        \n",
      "[1729] \"rep\"           \"repair\"        \"repay\"         \"repeal\"       \n",
      "[1733] \"repeat\"        \"replac\"        \"report\"        \"repres\"       \n",
      "[1737] \"represent\"     \"reproduct\"     \"republ\"        \"republican\"   \n",
      "[1741] \"request\"       \"requir\"        \"rescu\"         \"research\"     \n",
      "[1745] \"reserv\"        \"resid\"         \"resign\"        \"resolut\"      \n",
      "[1749] \"resolv\"        \"resourc\"       \"respect\"       \"respond\"      \n",
      "[1753] \"respons\"       \"rest\"          \"restaur\"       \"restor\"       \n",
      "[1757] \"restrict\"      \"result\"        \"retain\"        \"retir\"        \n",
      "[1761] \"retire\"        \"return\"        \"rev\"           \"reveal\"       \n",
      "[1765] \"revel\"         \"revenu\"        \"revers\"        \"review\"       \n",
      "[1769] \"revok\"         \"revolut\"       \"reward\"        \"rice\"         \n",
      "[1773] \"rich\"          \"rick\"          \"rid\"           \"ride\"         \n",
      "[1777] \"rifl\"          \"rig\"           \"right\"         \"rise\"         \n",
      "[1781] \"risk\"          \"riski\"         \"rival\"         \"river\"        \n",
      "[1785] \"road\"          \"roadway\"       \"rob\"           \"robberi\"      \n",
      "[1789] \"rock\"          \"roe\"           \"role\"          \"roll\"         \n",
      "[1793] \"room\"          \"rough\"         \"round\"         \"row\"          \n",
      "[1797] \"rug\"           \"rule\"          \"run\"           \"rural\"        \n",
      "[1801] \"rush\"          \"safe\"          \"safeti\"        \"salari\"       \n",
      "[1805] \"sale\"          \"sanction\"      \"sanctuari\"     \"sander\"       \n",
      "[1809] \"sandi\"         \"sandwich\"      \"save\"          \"say\"          \n",
      "[1813] \"scale\"         \"scandal\"       \"schedul\"       \"scheme\"       \n",
      "[1817] \"scholarship\"   \"school\"        \"schoolchild\"   \"scienc\"       \n",
      "[1821] \"scientif\"      \"scientist\"     \"score\"         \"screen\"       \n",
      "[1825] \"search\"        \"seat\"          \"seced\"         \"second\"       \n",
      "[1829] \"secret\"        \"secretari\"     \"sector\"        \"secur\"        \n",
      "[1833] \"see\"           \"seek\"          \"seem\"          \"select\"       \n",
      "[1837] \"sell\"          \"seller\"        \"sen\"           \"senat\"        \n",
      "[1841] \"send\"          \"senior\"        \"sentenc\"       \"separ\"        \n",
      "[1845] \"sequest\"       \"sequestr\"      \"serious\"       \"serv\"         \n",
      "[1849] \"servic\"        \"session\"       \"set\"           \"settl\"        \n",
      "[1853] \"seven\"         \"seventi\"       \"sever\"         \"sewer\"        \n",
      "[1857] \"sex\"           \"sexual\"        \"share\"         \"sharia\"       \n",
      "[1861] \"sheet\"         \"sheriff\"       \"shes\"          \"shift\"        \n",
      "[1865] \"ship\"          \"shock\"         \"shoot\"         \"shop\"         \n",
      "[1869] \"shore\"         \"short\"         \"shortfal\"      \"show\"         \n",
      "[1873] \"shrink\"        \"shut\"          \"shutdown\"      \"sic\"          \n",
      "[1877] \"sick\"          \"side\"          \"sidewalk\"      \"sign\"         \n",
      "[1881] \"signatur\"      \"signific\"      \"silent\"        \"similar\"      \n",
      "[1885] \"simpl\"         \"simpli\"        \"sinc\"          \"singl\"        \n",
      "[1889] \"sink\"          \"sit\"           \"site\"          \"situat\"       \n",
      "[1893] \"six\"           \"sixti\"         \"size\"          \"skill\"        \n",
      "[1897] \"skip\"          \"skyrocket\"     \"slash\"         \"slave\"        \n",
      "[1901] \"slaveri\"       \"slot\"          \"slow\"          \"slush\"        \n",
      "[1905] \"small\"         \"smart\"         \"smith\"         \"smoke\"        \n",
      "[1909] \"soar\"          \"soccer\"        \"social\"        \"socialist\"    \n",
      "[1913] \"societi\"       \"soda\"          \"solar\"         \"soldier\"      \n",
      "[1917] \"sole\"          \"solitari\"      \"solut\"         \"solv\"         \n",
      "[1921] \"somebodi\"      \"someon\"        \"someth\"        \"sometim\"      \n",
      "[1925] \"somewher\"      \"son\"           \"soon\"          \"sound\"        \n",
      "[1929] \"sourc\"         \"south\"         \"southern\"      \"southwest\"    \n",
      "[1933] \"soviet\"        \"space\"         \"speak\"         \"speaker\"      \n",
      "[1937] \"special\"       \"specif\"        \"speech\"        \"spend\"        \n",
      "[1941] \"spew\"          \"spi\"           \"spike\"         \"spill\"        \n",
      "[1945] \"split\"         \"sponsor\"       \"sport\"         \"spot\"         \n",
      "[1949] \"spous\"         \"spread\"        \"spree\"         \"spring\"       \n",
      "[1953] \"squar\"         \"st\"            \"stabil\"        \"stadium\"      \n",
      "[1957] \"staff\"         \"staffer\"       \"stage\"         \"stamp\"        \n",
      "[1961] \"stand\"         \"standard\"      \"star\"          \"start\"        \n",
      "[1965] \"startup\"       \"state\"         \"statement\"     \"statewid\"     \n",
      "[1969] \"statist\"       \"status\"        \"statut\"        \"stay\"         \n",
      "[1973] \"steal\"         \"stem\"          \"step\"          \"still\"        \n",
      "[1977] \"stimulus\"      \"stock\"         \"stone\"         \"stop\"         \n",
      "[1981] \"store\"         \"stori\"         \"storm\"         \"straight\"     \n",
      "[1985] \"strategi\"      \"stream\"        \"street\"        \"strict\"       \n",
      "[1989] \"strike\"        \"strip\"         \"strong\"        \"structur\"     \n",
      "[1993] \"struggl\"       \"student\"       \"studi\"         \"studio\"       \n",
      "[1997] \"stuff\"         \"subject\"       \"submit\"        \"subpoena\"     \n",
      "[2001] \"subprim\"       \"subsid\"        \"subsidi\"       \"substanc\"     \n",
      "[2005] \"substanti\"     \"substitut\"     \"succeed\"       \"success\"      \n",
      "[2009] \"sudden\"        \"sue\"           \"suffer\"        \"sugar\"        \n",
      "[2013] \"suggest\"       \"suicid\"        \"suit\"          \"summer\"       \n",
      "[2017] \"sun\"           \"super\"         \"superintend\"   \"superior\"     \n",
      "[2021] \"supplement\"    \"suppli\"        \"support\"       \"suppos\"       \n",
      "[2025] \"suprem\"        \"sure\"          \"surfac\"        \"surg\"         \n",
      "[2029] \"surgeri\"       \"surpass\"       \"surplus\"       \"surpris\"      \n",
      "[2033] \"surround\"      \"survey\"        \"surviv\"        \"suspect\"      \n",
      "[2037] \"suspend\"       \"swap\"          \"swear\"         \"switch\"       \n",
      "[2041] \"syndrom\"       \"system\"        \"tabl\"          \"take\"         \n",
      "[2045] \"takeov\"        \"talk\"          \"tank\"          \"tap\"          \n",
      "[2049] \"tape\"          \"target\"        \"tarp\"          \"tax\"          \n",
      "[2053] \"taxat\"         \"taxpay\"        \"tea\"           \"teach\"        \n",
      "[2057] \"teacher\"       \"team\"          \"tech\"          \"technic\"      \n",
      "[2061] \"technolog\"     \"ted\"           \"teen\"          \"teenag\"       \n",
      "[2065] \"televis\"       \"tell\"          \"temperatur\"    \"temporari\"    \n",
      "[2069] \"ten\"           \"tent\"          \"tenur\"         \"term\"         \n",
      "[2073] \"termin\"        \"terri\"         \"terribl\"       \"terror\"       \n",
      "[2077] \"terrorist\"     \"test\"          \"testifi\"       \"text\"         \n",
      "[2081] \"textbook\"      \"th\"            \"thank\"         \"thanksgiv\"    \n",
      "[2085] \"thing\"         \"think\"         \"though\"        \"thousand\"     \n",
      "[2089] \"threat\"        \"threaten\"      \"three\"         \"throughout\"   \n",
      "[2093] \"throw\"         \"ticket\"        \"tie\"           \"time\"         \n",
      "[2097] \"timelin\"       \"tobacco\"       \"today\"         \"togeth\"       \n",
      "[2101] \"toilet\"        \"toll\"          \"tom\"           \"tonight\"      \n",
      "[2105] \"tool\"          \"top\"           \"tortur\"        \"total\"        \n",
      "[2109] \"touch\"         \"tough\"         \"tour\"          \"tourism\"      \n",
      "[2113] \"toward\"        \"town\"          \"toxic\"         \"toy\"          \n",
      "[2117] \"track\"         \"trade\"         \"tradit\"        \"traffic\"      \n",
      "[2121] \"trail\"         \"train\"         \"transact\"      \"transfer\"     \n",
      "[2125] \"transit\"       \"translat\"      \"transpacif\"    \"transpar\"     \n",
      "[2129] \"transport\"     \"trap\"          \"travel\"        \"treasuri\"     \n",
      "[2133] \"treat\"         \"treati\"        \"treatment\"     \"tree\"         \n",
      "[2137] \"tremend\"       \"trend\"         \"tri\"           \"trial\"        \n",
      "[2141] \"tribe\"         \"trigger\"       \"trillion\"      \"trip\"         \n",
      "[2145] \"tripl\"         \"troop\"         \"troubl\"        \"truck\"        \n",
      "[2149] \"true\"          \"trump\"         \"trust\"         \"truth\"        \n",
      "[2153] \"tuition\"       \"tune\"          \"turbin\"        \"turkey\"       \n",
      "[2157] \"turn\"          \"turnout\"       \"turnpik\"       \"tweet\"        \n",
      "[2161] \"twenti\"        \"twice\"         \"twitter\"       \"two\"          \n",
      "[2165] \"type\"          \"typic\"         \"ultrasound\"    \"unabl\"        \n",
      "[2169] \"unarm\"         \"unconstitut\"   \"underag\"       \"undergo\"      \n",
      "[2173] \"understand\"    \"undocu\"        \"unemploy\"      \"unfair\"       \n",
      "[2177] \"unfund\"        \"uniform\"       \"unilater\"      \"uninsur\"      \n",
      "[2181] \"unintend\"      \"union\"         \"unit\"          \"univers\"      \n",
      "[2185] \"unlaw\"         \"unless\"        \"unlik\"         \"unpopular\"    \n",
      "[2189] \"unpreced\"      \"unsustain\"     \"upgrad\"        \"uranium\"      \n",
      "[2193] \"urban\"         \"urg\"           \"us\"            \"usag\"         \n",
      "[2197] \"use\"           \"user\"          \"usual\"         \"util\"         \n",
      "[2201] \"v\"             \"vacanc\"        \"vacant\"        \"vacat\"        \n",
      "[2205] \"vaccin\"        \"valley\"        \"valu\"          \"van\"          \n",
      "[2209] \"various\"       \"vast\"          \"vehicl\"        \"vehicular\"    \n",
      "[2213] \"verifi\"        \"version\"       \"vet\"           \"veteran\"      \n",
      "[2217] \"veto\"          \"via\"           \"viabl\"         \"vice\"         \n",
      "[2221] \"vicious\"       \"victim\"        \"victori\"       \"video\"        \n",
      "[2225] \"view\"          \"violat\"        \"violenc\"       \"violent\"      \n",
      "[2229] \"virtual\"       \"virus\"         \"visa\"          \"visit\"        \n",
      "[2233] \"visitor\"       \"vote\"          \"voter\"         \"voucher\"      \n",
      "[2237] \"vs\"            \"w\"             \"wade\"          \"wage\"         \n",
      "[2241] \"wait\"          \"waiv\"          \"waiver\"        \"walk\"         \n",
      "[2245] \"walker\"        \"wall\"          \"want\"          \"war\"          \n",
      "[2249] \"warm\"          \"warn\"          \"warrant\"       \"warren\"       \n",
      "[2253] \"wash\"          \"wast\"          \"watch\"         \"water\"        \n",
      "[2257] \"way\"           \"weak\"          \"weaken\"        \"wealth\"       \n",
      "[2261] \"wealthi\"       \"weapon\"        \"wear\"          \"weather\"      \n",
      "[2265] \"websit\"        \"wed\"           \"week\"          \"weekend\"      \n",
      "[2269] \"weigh\"         \"welcom\"        \"welfar\"        \"well\"         \n",
      "[2273] \"west\"          \"western\"       \"what\"          \"whatev\"       \n",
      "[2277] \"whatsoev\"      \"whether\"       \"white\"         \"whole\"        \n",
      "[2281] \"whop\"          \"whose\"         \"wide\"          \"wife\"         \n",
      "[2285] \"wild\"          \"wildlif\"       \"will\"          \"win\"          \n",
      "[2289] \"wind\"          \"window\"        \"winner\"        \"winter\"       \n",
      "[2293] \"wipe\"          \"wit\"           \"withdraw\"      \"within\"       \n",
      "[2297] \"without\"       \"woman\"         \"wont\"          \"word\"         \n",
      "[2301] \"work\"          \"worker\"        \"workforc\"      \"workman\"      \n",
      "[2305] \"workplac\"      \"world\"         \"worri\"         \"worth\"        \n",
      "[2309] \"wound\"         \"write\"         \"wrong\"         \"year\"         \n",
      "[2313] \"yes\"           \"yet\"           \"young\"         \"youth\"        \n",
      "[2317] \"zero\"          \"zip\"           \"zone\"         \n"
     ]
    }
   ],
   "source": [
    "print(model$vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0          1          2          3          4          5 \n",
      "0.08164689 0.19078856 0.16427076 0.20935101 0.19023029 0.16371249 \n"
     ]
    }
   ],
   "source": [
    "print(model$prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 2319 × 6 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>0</th><th scope=col>1</th><th scope=col>2</th><th scope=col>3</th><th scope=col>4</th><th scope=col>5</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td>0.0002843737</td><td>5.940006e-04</td><td>4.725526e-04</td><td>7.563820e-04</td><td>1.110803e-03</td><td>5.659767e-04</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.0002843737</td><td>3.712504e-04</td><td>3.150350e-04</td><td>5.042546e-04</td><td>3.471258e-04</td><td>5.659767e-04</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.0001421868</td><td>2.970003e-04</td><td>7.875876e-05</td><td>1.260637e-04</td><td>1.388503e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.0001421868</td><td>1.485001e-04</td><td>7.875876e-05</td><td>1.890955e-04</td><td>2.777006e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0.0001421868</td><td>1.485001e-04</td><td>1.575175e-04</td><td>6.303183e-05</td><td>2.082755e-04</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>abil</th><td>0.0001421868</td><td>3.712504e-04</td><td>3.937938e-04</td><td>1.260637e-04</td><td>2.777006e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>abl</th><td>0.0002843737</td><td>6.682507e-04</td><td>3.937938e-04</td><td>3.151592e-04</td><td>6.248264e-04</td><td>3.234153e-04</td></tr>\n",
       "\t<tr><th scope=row>abolish</th><td>0.0002843737</td><td>3.712504e-04</td><td>2.362763e-04</td><td>1.260637e-04</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>abort</th><td>0.0008531210</td><td>2.153252e-03</td><td>1.338899e-03</td><td>1.638828e-03</td><td>9.025271e-04</td><td>1.697930e-03</td></tr>\n",
       "\t<tr><th scope=row>absente</th><td>0.0001421868</td><td>1.485001e-04</td><td>7.875876e-05</td><td>1.260637e-04</td><td>1.388503e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>absolut</th><td>0.0004265605</td><td>2.227502e-04</td><td>7.875876e-05</td><td>1.890955e-04</td><td>2.777006e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>abus</th><td>0.0002843737</td><td>2.227502e-04</td><td>2.362763e-04</td><td>1.260637e-04</td><td>6.942516e-05</td><td>4.042691e-04</td></tr>\n",
       "\t<tr><th scope=row>academi</th><td>0.0001421868</td><td>7.425007e-05</td><td>2.362763e-04</td><td>6.303183e-05</td><td>1.388503e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>acceler</th><td>0.0001421868</td><td>1.485001e-04</td><td>7.875876e-05</td><td>1.260637e-04</td><td>1.388503e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>accept</th><td>0.0007109342</td><td>3.712504e-04</td><td>5.513113e-04</td><td>3.151592e-04</td><td>5.554013e-04</td><td>3.234153e-04</td></tr>\n",
       "\t<tr><th scope=row>access</th><td>0.0002843737</td><td>4.455004e-04</td><td>4.725526e-04</td><td>5.042546e-04</td><td>4.859761e-04</td><td>4.851229e-04</td></tr>\n",
       "\t<tr><th scope=row>accid</th><td>0.0001421868</td><td>7.425007e-05</td><td>7.875876e-05</td><td>3.781910e-04</td><td>3.471258e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>accident</th><td>0.0002843737</td><td>1.485001e-04</td><td>7.875876e-05</td><td>6.303183e-05</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>accommod</th><td>0.0001421868</td><td>7.425007e-05</td><td>2.362763e-04</td><td>6.303183e-05</td><td>2.082755e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>accord</th><td>0.0005687473</td><td>2.227502e-04</td><td>5.513113e-04</td><td>5.672865e-04</td><td>4.165510e-04</td><td>4.042691e-04</td></tr>\n",
       "\t<tr><th scope=row>account</th><td>0.0007109342</td><td>4.455004e-04</td><td>6.300701e-04</td><td>3.781910e-04</td><td>6.942516e-04</td><td>7.276843e-04</td></tr>\n",
       "\t<tr><th scope=row>accumul</th><td>0.0001421868</td><td>1.485001e-04</td><td>3.150350e-04</td><td>1.260637e-04</td><td>6.942516e-05</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>accus</th><td>0.0002843737</td><td>7.425007e-05</td><td>1.575175e-04</td><td>1.260637e-04</td><td>1.388503e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>achiev</th><td>0.0001421868</td><td>1.485001e-04</td><td>2.362763e-04</td><td>1.260637e-04</td><td>1.388503e-04</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>acknowledg</th><td>0.0001421868</td><td>7.425007e-05</td><td>1.575175e-04</td><td>1.890955e-04</td><td>6.942516e-05</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>acorn</th><td>0.0001421868</td><td>2.970003e-04</td><td>1.575175e-04</td><td>1.260637e-04</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>acr</th><td>0.0001421868</td><td>2.227502e-04</td><td>7.875876e-05</td><td>2.521273e-04</td><td>2.082755e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>across</th><td>0.0005687473</td><td>2.970003e-04</td><td>3.150350e-04</td><td>5.042546e-04</td><td>5.554013e-04</td><td>6.468305e-04</td></tr>\n",
       "\t<tr><th scope=row>act</th><td>0.0009953078</td><td>1.410751e-03</td><td>1.023864e-03</td><td>1.764891e-03</td><td>1.527354e-03</td><td>8.085382e-04</td></tr>\n",
       "\t<tr><th scope=row>action</th><td>0.0005687473</td><td>2.970003e-04</td><td>4.725526e-04</td><td>3.781910e-04</td><td>4.859761e-04</td><td>4.042691e-04</td></tr>\n",
       "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><th scope=row>window</th><td>0.0004265605</td><td>1.485001e-04</td><td>7.875876e-05</td><td>6.303183e-05</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>winner</th><td>0.0001421868</td><td>1.485001e-04</td><td>1.575175e-04</td><td>6.303183e-05</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>winter</th><td>0.0001421868</td><td>7.425007e-05</td><td>1.575175e-04</td><td>6.303183e-05</td><td>2.082755e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>wipe</th><td>0.0001421868</td><td>1.485001e-04</td><td>1.575175e-04</td><td>3.781910e-04</td><td>1.388503e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>wit</th><td>0.0001421868</td><td>7.425007e-05</td><td>1.575175e-04</td><td>1.260637e-04</td><td>1.388503e-04</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>withdraw</th><td>0.0001421868</td><td>2.970003e-04</td><td>3.150350e-04</td><td>1.890955e-04</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>within</th><td>0.0001421868</td><td>2.970003e-04</td><td>1.575175e-04</td><td>1.890955e-04</td><td>5.554013e-04</td><td>6.468305e-04</td></tr>\n",
       "\t<tr><th scope=row>without</th><td>0.0007109342</td><td>1.410751e-03</td><td>1.023864e-03</td><td>1.638828e-03</td><td>9.719522e-04</td><td>1.212807e-03</td></tr>\n",
       "\t<tr><th scope=row>woman</th><td>0.0012796815</td><td>1.856252e-03</td><td>2.756557e-03</td><td>1.701859e-03</td><td>1.735629e-03</td><td>1.697930e-03</td></tr>\n",
       "\t<tr><th scope=row>wont</th><td>0.0004265605</td><td>2.970003e-04</td><td>3.937938e-04</td><td>2.521273e-04</td><td>3.471258e-04</td><td>3.234153e-04</td></tr>\n",
       "\t<tr><th scope=row>word</th><td>0.0007109342</td><td>2.970003e-04</td><td>1.575175e-04</td><td>3.151592e-04</td><td>3.471258e-04</td><td>8.085382e-04</td></tr>\n",
       "\t<tr><th scope=row>work</th><td>0.0012796815</td><td>2.895753e-03</td><td>2.677798e-03</td><td>2.521273e-03</td><td>1.943904e-03</td><td>1.455369e-03</td></tr>\n",
       "\t<tr><th scope=row>worker</th><td>0.0004265605</td><td>9.652510e-04</td><td>1.575175e-03</td><td>1.575796e-03</td><td>1.596779e-03</td><td>8.085382e-04</td></tr>\n",
       "\t<tr><th scope=row>workforc</th><td>0.0001421868</td><td>2.227502e-04</td><td>3.150350e-04</td><td>5.042546e-04</td><td>4.165510e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>workman</th><td>0.0001421868</td><td>1.485001e-04</td><td>7.875876e-05</td><td>1.260637e-04</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>workplac</th><td>0.0002843737</td><td>2.227502e-04</td><td>7.875876e-05</td><td>1.260637e-04</td><td>6.942516e-05</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>world</th><td>0.0022749893</td><td>1.336501e-03</td><td>7.875876e-04</td><td>2.017019e-03</td><td>2.152180e-03</td><td>1.697930e-03</td></tr>\n",
       "\t<tr><th scope=row>worri</th><td>0.0002843737</td><td>7.425007e-05</td><td>7.875876e-05</td><td>1.260637e-04</td><td>1.388503e-04</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>worth</th><td>0.0002843737</td><td>2.227502e-04</td><td>7.875876e-05</td><td>3.781910e-04</td><td>4.859761e-04</td><td>7.276843e-04</td></tr>\n",
       "\t<tr><th scope=row>wound</th><td>0.0001421868</td><td>7.425007e-05</td><td>1.575175e-04</td><td>2.521273e-04</td><td>6.942516e-05</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>write</th><td>0.0007109342</td><td>5.197505e-04</td><td>4.725526e-04</td><td>6.933501e-04</td><td>4.859761e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>wrong</th><td>0.0001421868</td><td>2.970003e-04</td><td>1.575175e-04</td><td>1.890955e-04</td><td>4.165510e-04</td><td>2.425614e-04</td></tr>\n",
       "\t<tr><th scope=row>year</th><td>0.0051187260</td><td>1.009801e-02</td><td>1.015988e-02</td><td>1.084147e-02</td><td>1.346848e-02</td><td>1.261320e-02</td></tr>\n",
       "\t<tr><th scope=row>yes</th><td>0.0002843737</td><td>2.970003e-04</td><td>7.875876e-05</td><td>6.303183e-05</td><td>6.942516e-05</td><td>8.085382e-05</td></tr>\n",
       "\t<tr><th scope=row>yet</th><td>0.0005687473</td><td>2.227502e-04</td><td>4.725526e-04</td><td>3.781910e-04</td><td>4.859761e-04</td><td>3.234153e-04</td></tr>\n",
       "\t<tr><th scope=row>young</th><td>0.0002843737</td><td>5.197505e-04</td><td>2.362763e-04</td><td>6.303183e-04</td><td>3.471258e-04</td><td>6.468305e-04</td></tr>\n",
       "\t<tr><th scope=row>youth</th><td>0.0001421868</td><td>7.425007e-05</td><td>2.362763e-04</td><td>1.890955e-04</td><td>2.777006e-04</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>zero</th><td>0.0007109342</td><td>2.970003e-04</td><td>6.300701e-04</td><td>2.521273e-04</td><td>4.165510e-04</td><td>8.893920e-04</td></tr>\n",
       "\t<tr><th scope=row>zip</th><td>0.0001421868</td><td>1.485001e-04</td><td>1.575175e-04</td><td>6.303183e-05</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "\t<tr><th scope=row>zone</th><td>0.0002843737</td><td>7.425007e-05</td><td>1.575175e-04</td><td>4.412228e-04</td><td>6.942516e-05</td><td>1.617076e-04</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 2319 × 6 of type dbl\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & 0 & 1 & 2 & 3 & 4 & 5\\\\\n",
       "\\hline\n",
       "\t2 & 0.0002843737 & 5.940006e-04 & 4.725526e-04 & 7.563820e-04 & 1.110803e-03 & 5.659767e-04\\\\\n",
       "\t3 & 0.0002843737 & 3.712504e-04 & 3.150350e-04 & 5.042546e-04 & 3.471258e-04 & 5.659767e-04\\\\\n",
       "\t4 & 0.0001421868 & 2.970003e-04 & 7.875876e-05 & 1.260637e-04 & 1.388503e-04 & 2.425614e-04\\\\\n",
       "\t5 & 0.0001421868 & 1.485001e-04 & 7.875876e-05 & 1.890955e-04 & 2.777006e-04 & 2.425614e-04\\\\\n",
       "\t6 & 0.0001421868 & 1.485001e-04 & 1.575175e-04 & 6.303183e-05 & 2.082755e-04 & 1.617076e-04\\\\\n",
       "\tabil & 0.0001421868 & 3.712504e-04 & 3.937938e-04 & 1.260637e-04 & 2.777006e-04 & 8.085382e-05\\\\\n",
       "\tabl & 0.0002843737 & 6.682507e-04 & 3.937938e-04 & 3.151592e-04 & 6.248264e-04 & 3.234153e-04\\\\\n",
       "\tabolish & 0.0002843737 & 3.712504e-04 & 2.362763e-04 & 1.260637e-04 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\tabort & 0.0008531210 & 2.153252e-03 & 1.338899e-03 & 1.638828e-03 & 9.025271e-04 & 1.697930e-03\\\\\n",
       "\tabsente & 0.0001421868 & 1.485001e-04 & 7.875876e-05 & 1.260637e-04 & 1.388503e-04 & 8.085382e-05\\\\\n",
       "\tabsolut & 0.0004265605 & 2.227502e-04 & 7.875876e-05 & 1.890955e-04 & 2.777006e-04 & 2.425614e-04\\\\\n",
       "\tabus & 0.0002843737 & 2.227502e-04 & 2.362763e-04 & 1.260637e-04 & 6.942516e-05 & 4.042691e-04\\\\\n",
       "\tacademi & 0.0001421868 & 7.425007e-05 & 2.362763e-04 & 6.303183e-05 & 1.388503e-04 & 8.085382e-05\\\\\n",
       "\tacceler & 0.0001421868 & 1.485001e-04 & 7.875876e-05 & 1.260637e-04 & 1.388503e-04 & 8.085382e-05\\\\\n",
       "\taccept & 0.0007109342 & 3.712504e-04 & 5.513113e-04 & 3.151592e-04 & 5.554013e-04 & 3.234153e-04\\\\\n",
       "\taccess & 0.0002843737 & 4.455004e-04 & 4.725526e-04 & 5.042546e-04 & 4.859761e-04 & 4.851229e-04\\\\\n",
       "\taccid & 0.0001421868 & 7.425007e-05 & 7.875876e-05 & 3.781910e-04 & 3.471258e-04 & 2.425614e-04\\\\\n",
       "\taccident & 0.0002843737 & 1.485001e-04 & 7.875876e-05 & 6.303183e-05 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\taccommod & 0.0001421868 & 7.425007e-05 & 2.362763e-04 & 6.303183e-05 & 2.082755e-04 & 8.085382e-05\\\\\n",
       "\taccord & 0.0005687473 & 2.227502e-04 & 5.513113e-04 & 5.672865e-04 & 4.165510e-04 & 4.042691e-04\\\\\n",
       "\taccount & 0.0007109342 & 4.455004e-04 & 6.300701e-04 & 3.781910e-04 & 6.942516e-04 & 7.276843e-04\\\\\n",
       "\taccumul & 0.0001421868 & 1.485001e-04 & 3.150350e-04 & 1.260637e-04 & 6.942516e-05 & 8.085382e-05\\\\\n",
       "\taccus & 0.0002843737 & 7.425007e-05 & 1.575175e-04 & 1.260637e-04 & 1.388503e-04 & 2.425614e-04\\\\\n",
       "\tachiev & 0.0001421868 & 1.485001e-04 & 2.362763e-04 & 1.260637e-04 & 1.388503e-04 & 1.617076e-04\\\\\n",
       "\tacknowledg & 0.0001421868 & 7.425007e-05 & 1.575175e-04 & 1.890955e-04 & 6.942516e-05 & 8.085382e-05\\\\\n",
       "\tacorn & 0.0001421868 & 2.970003e-04 & 1.575175e-04 & 1.260637e-04 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\tacr & 0.0001421868 & 2.227502e-04 & 7.875876e-05 & 2.521273e-04 & 2.082755e-04 & 8.085382e-05\\\\\n",
       "\tacross & 0.0005687473 & 2.970003e-04 & 3.150350e-04 & 5.042546e-04 & 5.554013e-04 & 6.468305e-04\\\\\n",
       "\tact & 0.0009953078 & 1.410751e-03 & 1.023864e-03 & 1.764891e-03 & 1.527354e-03 & 8.085382e-04\\\\\n",
       "\taction & 0.0005687473 & 2.970003e-04 & 4.725526e-04 & 3.781910e-04 & 4.859761e-04 & 4.042691e-04\\\\\n",
       "\t⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\twindow & 0.0004265605 & 1.485001e-04 & 7.875876e-05 & 6.303183e-05 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\twinner & 0.0001421868 & 1.485001e-04 & 1.575175e-04 & 6.303183e-05 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\twinter & 0.0001421868 & 7.425007e-05 & 1.575175e-04 & 6.303183e-05 & 2.082755e-04 & 8.085382e-05\\\\\n",
       "\twipe & 0.0001421868 & 1.485001e-04 & 1.575175e-04 & 3.781910e-04 & 1.388503e-04 & 8.085382e-05\\\\\n",
       "\twit & 0.0001421868 & 7.425007e-05 & 1.575175e-04 & 1.260637e-04 & 1.388503e-04 & 1.617076e-04\\\\\n",
       "\twithdraw & 0.0001421868 & 2.970003e-04 & 3.150350e-04 & 1.890955e-04 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\twithin & 0.0001421868 & 2.970003e-04 & 1.575175e-04 & 1.890955e-04 & 5.554013e-04 & 6.468305e-04\\\\\n",
       "\twithout & 0.0007109342 & 1.410751e-03 & 1.023864e-03 & 1.638828e-03 & 9.719522e-04 & 1.212807e-03\\\\\n",
       "\twoman & 0.0012796815 & 1.856252e-03 & 2.756557e-03 & 1.701859e-03 & 1.735629e-03 & 1.697930e-03\\\\\n",
       "\twont & 0.0004265605 & 2.970003e-04 & 3.937938e-04 & 2.521273e-04 & 3.471258e-04 & 3.234153e-04\\\\\n",
       "\tword & 0.0007109342 & 2.970003e-04 & 1.575175e-04 & 3.151592e-04 & 3.471258e-04 & 8.085382e-04\\\\\n",
       "\twork & 0.0012796815 & 2.895753e-03 & 2.677798e-03 & 2.521273e-03 & 1.943904e-03 & 1.455369e-03\\\\\n",
       "\tworker & 0.0004265605 & 9.652510e-04 & 1.575175e-03 & 1.575796e-03 & 1.596779e-03 & 8.085382e-04\\\\\n",
       "\tworkforc & 0.0001421868 & 2.227502e-04 & 3.150350e-04 & 5.042546e-04 & 4.165510e-04 & 2.425614e-04\\\\\n",
       "\tworkman & 0.0001421868 & 1.485001e-04 & 7.875876e-05 & 1.260637e-04 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\tworkplac & 0.0002843737 & 2.227502e-04 & 7.875876e-05 & 1.260637e-04 & 6.942516e-05 & 8.085382e-05\\\\\n",
       "\tworld & 0.0022749893 & 1.336501e-03 & 7.875876e-04 & 2.017019e-03 & 2.152180e-03 & 1.697930e-03\\\\\n",
       "\tworri & 0.0002843737 & 7.425007e-05 & 7.875876e-05 & 1.260637e-04 & 1.388503e-04 & 8.085382e-05\\\\\n",
       "\tworth & 0.0002843737 & 2.227502e-04 & 7.875876e-05 & 3.781910e-04 & 4.859761e-04 & 7.276843e-04\\\\\n",
       "\twound & 0.0001421868 & 7.425007e-05 & 1.575175e-04 & 2.521273e-04 & 6.942516e-05 & 8.085382e-05\\\\\n",
       "\twrite & 0.0007109342 & 5.197505e-04 & 4.725526e-04 & 6.933501e-04 & 4.859761e-04 & 2.425614e-04\\\\\n",
       "\twrong & 0.0001421868 & 2.970003e-04 & 1.575175e-04 & 1.890955e-04 & 4.165510e-04 & 2.425614e-04\\\\\n",
       "\tyear & 0.0051187260 & 1.009801e-02 & 1.015988e-02 & 1.084147e-02 & 1.346848e-02 & 1.261320e-02\\\\\n",
       "\tyes & 0.0002843737 & 2.970003e-04 & 7.875876e-05 & 6.303183e-05 & 6.942516e-05 & 8.085382e-05\\\\\n",
       "\tyet & 0.0005687473 & 2.227502e-04 & 4.725526e-04 & 3.781910e-04 & 4.859761e-04 & 3.234153e-04\\\\\n",
       "\tyoung & 0.0002843737 & 5.197505e-04 & 2.362763e-04 & 6.303183e-04 & 3.471258e-04 & 6.468305e-04\\\\\n",
       "\tyouth & 0.0001421868 & 7.425007e-05 & 2.362763e-04 & 1.890955e-04 & 2.777006e-04 & 1.617076e-04\\\\\n",
       "\tzero & 0.0007109342 & 2.970003e-04 & 6.300701e-04 & 2.521273e-04 & 4.165510e-04 & 8.893920e-04\\\\\n",
       "\tzip & 0.0001421868 & 1.485001e-04 & 1.575175e-04 & 6.303183e-05 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\tzone & 0.0002843737 & 7.425007e-05 & 1.575175e-04 & 4.412228e-04 & 6.942516e-05 & 1.617076e-04\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 2319 × 6 of type dbl\n",
       "\n",
       "| <!--/--> | 0 | 1 | 2 | 3 | 4 | 5 |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 2 | 0.0002843737 | 5.940006e-04 | 4.725526e-04 | 7.563820e-04 | 1.110803e-03 | 5.659767e-04 |\n",
       "| 3 | 0.0002843737 | 3.712504e-04 | 3.150350e-04 | 5.042546e-04 | 3.471258e-04 | 5.659767e-04 |\n",
       "| 4 | 0.0001421868 | 2.970003e-04 | 7.875876e-05 | 1.260637e-04 | 1.388503e-04 | 2.425614e-04 |\n",
       "| 5 | 0.0001421868 | 1.485001e-04 | 7.875876e-05 | 1.890955e-04 | 2.777006e-04 | 2.425614e-04 |\n",
       "| 6 | 0.0001421868 | 1.485001e-04 | 1.575175e-04 | 6.303183e-05 | 2.082755e-04 | 1.617076e-04 |\n",
       "| abil | 0.0001421868 | 3.712504e-04 | 3.937938e-04 | 1.260637e-04 | 2.777006e-04 | 8.085382e-05 |\n",
       "| abl | 0.0002843737 | 6.682507e-04 | 3.937938e-04 | 3.151592e-04 | 6.248264e-04 | 3.234153e-04 |\n",
       "| abolish | 0.0002843737 | 3.712504e-04 | 2.362763e-04 | 1.260637e-04 | 6.942516e-05 | 1.617076e-04 |\n",
       "| abort | 0.0008531210 | 2.153252e-03 | 1.338899e-03 | 1.638828e-03 | 9.025271e-04 | 1.697930e-03 |\n",
       "| absente | 0.0001421868 | 1.485001e-04 | 7.875876e-05 | 1.260637e-04 | 1.388503e-04 | 8.085382e-05 |\n",
       "| absolut | 0.0004265605 | 2.227502e-04 | 7.875876e-05 | 1.890955e-04 | 2.777006e-04 | 2.425614e-04 |\n",
       "| abus | 0.0002843737 | 2.227502e-04 | 2.362763e-04 | 1.260637e-04 | 6.942516e-05 | 4.042691e-04 |\n",
       "| academi | 0.0001421868 | 7.425007e-05 | 2.362763e-04 | 6.303183e-05 | 1.388503e-04 | 8.085382e-05 |\n",
       "| acceler | 0.0001421868 | 1.485001e-04 | 7.875876e-05 | 1.260637e-04 | 1.388503e-04 | 8.085382e-05 |\n",
       "| accept | 0.0007109342 | 3.712504e-04 | 5.513113e-04 | 3.151592e-04 | 5.554013e-04 | 3.234153e-04 |\n",
       "| access | 0.0002843737 | 4.455004e-04 | 4.725526e-04 | 5.042546e-04 | 4.859761e-04 | 4.851229e-04 |\n",
       "| accid | 0.0001421868 | 7.425007e-05 | 7.875876e-05 | 3.781910e-04 | 3.471258e-04 | 2.425614e-04 |\n",
       "| accident | 0.0002843737 | 1.485001e-04 | 7.875876e-05 | 6.303183e-05 | 6.942516e-05 | 1.617076e-04 |\n",
       "| accommod | 0.0001421868 | 7.425007e-05 | 2.362763e-04 | 6.303183e-05 | 2.082755e-04 | 8.085382e-05 |\n",
       "| accord | 0.0005687473 | 2.227502e-04 | 5.513113e-04 | 5.672865e-04 | 4.165510e-04 | 4.042691e-04 |\n",
       "| account | 0.0007109342 | 4.455004e-04 | 6.300701e-04 | 3.781910e-04 | 6.942516e-04 | 7.276843e-04 |\n",
       "| accumul | 0.0001421868 | 1.485001e-04 | 3.150350e-04 | 1.260637e-04 | 6.942516e-05 | 8.085382e-05 |\n",
       "| accus | 0.0002843737 | 7.425007e-05 | 1.575175e-04 | 1.260637e-04 | 1.388503e-04 | 2.425614e-04 |\n",
       "| achiev | 0.0001421868 | 1.485001e-04 | 2.362763e-04 | 1.260637e-04 | 1.388503e-04 | 1.617076e-04 |\n",
       "| acknowledg | 0.0001421868 | 7.425007e-05 | 1.575175e-04 | 1.890955e-04 | 6.942516e-05 | 8.085382e-05 |\n",
       "| acorn | 0.0001421868 | 2.970003e-04 | 1.575175e-04 | 1.260637e-04 | 6.942516e-05 | 1.617076e-04 |\n",
       "| acr | 0.0001421868 | 2.227502e-04 | 7.875876e-05 | 2.521273e-04 | 2.082755e-04 | 8.085382e-05 |\n",
       "| across | 0.0005687473 | 2.970003e-04 | 3.150350e-04 | 5.042546e-04 | 5.554013e-04 | 6.468305e-04 |\n",
       "| act | 0.0009953078 | 1.410751e-03 | 1.023864e-03 | 1.764891e-03 | 1.527354e-03 | 8.085382e-04 |\n",
       "| action | 0.0005687473 | 2.970003e-04 | 4.725526e-04 | 3.781910e-04 | 4.859761e-04 | 4.042691e-04 |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| window | 0.0004265605 | 1.485001e-04 | 7.875876e-05 | 6.303183e-05 | 6.942516e-05 | 1.617076e-04 |\n",
       "| winner | 0.0001421868 | 1.485001e-04 | 1.575175e-04 | 6.303183e-05 | 6.942516e-05 | 1.617076e-04 |\n",
       "| winter | 0.0001421868 | 7.425007e-05 | 1.575175e-04 | 6.303183e-05 | 2.082755e-04 | 8.085382e-05 |\n",
       "| wipe | 0.0001421868 | 1.485001e-04 | 1.575175e-04 | 3.781910e-04 | 1.388503e-04 | 8.085382e-05 |\n",
       "| wit | 0.0001421868 | 7.425007e-05 | 1.575175e-04 | 1.260637e-04 | 1.388503e-04 | 1.617076e-04 |\n",
       "| withdraw | 0.0001421868 | 2.970003e-04 | 3.150350e-04 | 1.890955e-04 | 6.942516e-05 | 1.617076e-04 |\n",
       "| within | 0.0001421868 | 2.970003e-04 | 1.575175e-04 | 1.890955e-04 | 5.554013e-04 | 6.468305e-04 |\n",
       "| without | 0.0007109342 | 1.410751e-03 | 1.023864e-03 | 1.638828e-03 | 9.719522e-04 | 1.212807e-03 |\n",
       "| woman | 0.0012796815 | 1.856252e-03 | 2.756557e-03 | 1.701859e-03 | 1.735629e-03 | 1.697930e-03 |\n",
       "| wont | 0.0004265605 | 2.970003e-04 | 3.937938e-04 | 2.521273e-04 | 3.471258e-04 | 3.234153e-04 |\n",
       "| word | 0.0007109342 | 2.970003e-04 | 1.575175e-04 | 3.151592e-04 | 3.471258e-04 | 8.085382e-04 |\n",
       "| work | 0.0012796815 | 2.895753e-03 | 2.677798e-03 | 2.521273e-03 | 1.943904e-03 | 1.455369e-03 |\n",
       "| worker | 0.0004265605 | 9.652510e-04 | 1.575175e-03 | 1.575796e-03 | 1.596779e-03 | 8.085382e-04 |\n",
       "| workforc | 0.0001421868 | 2.227502e-04 | 3.150350e-04 | 5.042546e-04 | 4.165510e-04 | 2.425614e-04 |\n",
       "| workman | 0.0001421868 | 1.485001e-04 | 7.875876e-05 | 1.260637e-04 | 6.942516e-05 | 1.617076e-04 |\n",
       "| workplac | 0.0002843737 | 2.227502e-04 | 7.875876e-05 | 1.260637e-04 | 6.942516e-05 | 8.085382e-05 |\n",
       "| world | 0.0022749893 | 1.336501e-03 | 7.875876e-04 | 2.017019e-03 | 2.152180e-03 | 1.697930e-03 |\n",
       "| worri | 0.0002843737 | 7.425007e-05 | 7.875876e-05 | 1.260637e-04 | 1.388503e-04 | 8.085382e-05 |\n",
       "| worth | 0.0002843737 | 2.227502e-04 | 7.875876e-05 | 3.781910e-04 | 4.859761e-04 | 7.276843e-04 |\n",
       "| wound | 0.0001421868 | 7.425007e-05 | 1.575175e-04 | 2.521273e-04 | 6.942516e-05 | 8.085382e-05 |\n",
       "| write | 0.0007109342 | 5.197505e-04 | 4.725526e-04 | 6.933501e-04 | 4.859761e-04 | 2.425614e-04 |\n",
       "| wrong | 0.0001421868 | 2.970003e-04 | 1.575175e-04 | 1.890955e-04 | 4.165510e-04 | 2.425614e-04 |\n",
       "| year | 0.0051187260 | 1.009801e-02 | 1.015988e-02 | 1.084147e-02 | 1.346848e-02 | 1.261320e-02 |\n",
       "| yes | 0.0002843737 | 2.970003e-04 | 7.875876e-05 | 6.303183e-05 | 6.942516e-05 | 8.085382e-05 |\n",
       "| yet | 0.0005687473 | 2.227502e-04 | 4.725526e-04 | 3.781910e-04 | 4.859761e-04 | 3.234153e-04 |\n",
       "| young | 0.0002843737 | 5.197505e-04 | 2.362763e-04 | 6.303183e-04 | 3.471258e-04 | 6.468305e-04 |\n",
       "| youth | 0.0001421868 | 7.425007e-05 | 2.362763e-04 | 1.890955e-04 | 2.777006e-04 | 1.617076e-04 |\n",
       "| zero | 0.0007109342 | 2.970003e-04 | 6.300701e-04 | 2.521273e-04 | 4.165510e-04 | 8.893920e-04 |\n",
       "| zip | 0.0001421868 | 1.485001e-04 | 1.575175e-04 | 6.303183e-05 | 6.942516e-05 | 1.617076e-04 |\n",
       "| zone | 0.0002843737 | 7.425007e-05 | 1.575175e-04 | 4.412228e-04 | 6.942516e-05 | 1.617076e-04 |\n",
       "\n"
      ],
      "text/plain": [
       "           0            1            2            3            4           \n",
       "2          0.0002843737 5.940006e-04 4.725526e-04 7.563820e-04 1.110803e-03\n",
       "3          0.0002843737 3.712504e-04 3.150350e-04 5.042546e-04 3.471258e-04\n",
       "4          0.0001421868 2.970003e-04 7.875876e-05 1.260637e-04 1.388503e-04\n",
       "5          0.0001421868 1.485001e-04 7.875876e-05 1.890955e-04 2.777006e-04\n",
       "6          0.0001421868 1.485001e-04 1.575175e-04 6.303183e-05 2.082755e-04\n",
       "abil       0.0001421868 3.712504e-04 3.937938e-04 1.260637e-04 2.777006e-04\n",
       "abl        0.0002843737 6.682507e-04 3.937938e-04 3.151592e-04 6.248264e-04\n",
       "abolish    0.0002843737 3.712504e-04 2.362763e-04 1.260637e-04 6.942516e-05\n",
       "abort      0.0008531210 2.153252e-03 1.338899e-03 1.638828e-03 9.025271e-04\n",
       "absente    0.0001421868 1.485001e-04 7.875876e-05 1.260637e-04 1.388503e-04\n",
       "absolut    0.0004265605 2.227502e-04 7.875876e-05 1.890955e-04 2.777006e-04\n",
       "abus       0.0002843737 2.227502e-04 2.362763e-04 1.260637e-04 6.942516e-05\n",
       "academi    0.0001421868 7.425007e-05 2.362763e-04 6.303183e-05 1.388503e-04\n",
       "acceler    0.0001421868 1.485001e-04 7.875876e-05 1.260637e-04 1.388503e-04\n",
       "accept     0.0007109342 3.712504e-04 5.513113e-04 3.151592e-04 5.554013e-04\n",
       "access     0.0002843737 4.455004e-04 4.725526e-04 5.042546e-04 4.859761e-04\n",
       "accid      0.0001421868 7.425007e-05 7.875876e-05 3.781910e-04 3.471258e-04\n",
       "accident   0.0002843737 1.485001e-04 7.875876e-05 6.303183e-05 6.942516e-05\n",
       "accommod   0.0001421868 7.425007e-05 2.362763e-04 6.303183e-05 2.082755e-04\n",
       "accord     0.0005687473 2.227502e-04 5.513113e-04 5.672865e-04 4.165510e-04\n",
       "account    0.0007109342 4.455004e-04 6.300701e-04 3.781910e-04 6.942516e-04\n",
       "accumul    0.0001421868 1.485001e-04 3.150350e-04 1.260637e-04 6.942516e-05\n",
       "accus      0.0002843737 7.425007e-05 1.575175e-04 1.260637e-04 1.388503e-04\n",
       "achiev     0.0001421868 1.485001e-04 2.362763e-04 1.260637e-04 1.388503e-04\n",
       "acknowledg 0.0001421868 7.425007e-05 1.575175e-04 1.890955e-04 6.942516e-05\n",
       "acorn      0.0001421868 2.970003e-04 1.575175e-04 1.260637e-04 6.942516e-05\n",
       "acr        0.0001421868 2.227502e-04 7.875876e-05 2.521273e-04 2.082755e-04\n",
       "across     0.0005687473 2.970003e-04 3.150350e-04 5.042546e-04 5.554013e-04\n",
       "act        0.0009953078 1.410751e-03 1.023864e-03 1.764891e-03 1.527354e-03\n",
       "action     0.0005687473 2.970003e-04 4.725526e-04 3.781910e-04 4.859761e-04\n",
       "⋮          ⋮            ⋮            ⋮            ⋮            ⋮           \n",
       "window     0.0004265605 1.485001e-04 7.875876e-05 6.303183e-05 6.942516e-05\n",
       "winner     0.0001421868 1.485001e-04 1.575175e-04 6.303183e-05 6.942516e-05\n",
       "winter     0.0001421868 7.425007e-05 1.575175e-04 6.303183e-05 2.082755e-04\n",
       "wipe       0.0001421868 1.485001e-04 1.575175e-04 3.781910e-04 1.388503e-04\n",
       "wit        0.0001421868 7.425007e-05 1.575175e-04 1.260637e-04 1.388503e-04\n",
       "withdraw   0.0001421868 2.970003e-04 3.150350e-04 1.890955e-04 6.942516e-05\n",
       "within     0.0001421868 2.970003e-04 1.575175e-04 1.890955e-04 5.554013e-04\n",
       "without    0.0007109342 1.410751e-03 1.023864e-03 1.638828e-03 9.719522e-04\n",
       "woman      0.0012796815 1.856252e-03 2.756557e-03 1.701859e-03 1.735629e-03\n",
       "wont       0.0004265605 2.970003e-04 3.937938e-04 2.521273e-04 3.471258e-04\n",
       "word       0.0007109342 2.970003e-04 1.575175e-04 3.151592e-04 3.471258e-04\n",
       "work       0.0012796815 2.895753e-03 2.677798e-03 2.521273e-03 1.943904e-03\n",
       "worker     0.0004265605 9.652510e-04 1.575175e-03 1.575796e-03 1.596779e-03\n",
       "workforc   0.0001421868 2.227502e-04 3.150350e-04 5.042546e-04 4.165510e-04\n",
       "workman    0.0001421868 1.485001e-04 7.875876e-05 1.260637e-04 6.942516e-05\n",
       "workplac   0.0002843737 2.227502e-04 7.875876e-05 1.260637e-04 6.942516e-05\n",
       "world      0.0022749893 1.336501e-03 7.875876e-04 2.017019e-03 2.152180e-03\n",
       "worri      0.0002843737 7.425007e-05 7.875876e-05 1.260637e-04 1.388503e-04\n",
       "worth      0.0002843737 2.227502e-04 7.875876e-05 3.781910e-04 4.859761e-04\n",
       "wound      0.0001421868 7.425007e-05 1.575175e-04 2.521273e-04 6.942516e-05\n",
       "write      0.0007109342 5.197505e-04 4.725526e-04 6.933501e-04 4.859761e-04\n",
       "wrong      0.0001421868 2.970003e-04 1.575175e-04 1.890955e-04 4.165510e-04\n",
       "year       0.0051187260 1.009801e-02 1.015988e-02 1.084147e-02 1.346848e-02\n",
       "yes        0.0002843737 2.970003e-04 7.875876e-05 6.303183e-05 6.942516e-05\n",
       "yet        0.0005687473 2.227502e-04 4.725526e-04 3.781910e-04 4.859761e-04\n",
       "young      0.0002843737 5.197505e-04 2.362763e-04 6.303183e-04 3.471258e-04\n",
       "youth      0.0001421868 7.425007e-05 2.362763e-04 1.890955e-04 2.777006e-04\n",
       "zero       0.0007109342 2.970003e-04 6.300701e-04 2.521273e-04 4.165510e-04\n",
       "zip        0.0001421868 1.485001e-04 1.575175e-04 6.303183e-05 6.942516e-05\n",
       "zone       0.0002843737 7.425007e-05 1.575175e-04 4.412228e-04 6.942516e-05\n",
       "           5           \n",
       "2          5.659767e-04\n",
       "3          5.659767e-04\n",
       "4          2.425614e-04\n",
       "5          2.425614e-04\n",
       "6          1.617076e-04\n",
       "abil       8.085382e-05\n",
       "abl        3.234153e-04\n",
       "abolish    1.617076e-04\n",
       "abort      1.697930e-03\n",
       "absente    8.085382e-05\n",
       "absolut    2.425614e-04\n",
       "abus       4.042691e-04\n",
       "academi    8.085382e-05\n",
       "acceler    8.085382e-05\n",
       "accept     3.234153e-04\n",
       "access     4.851229e-04\n",
       "accid      2.425614e-04\n",
       "accident   1.617076e-04\n",
       "accommod   8.085382e-05\n",
       "accord     4.042691e-04\n",
       "account    7.276843e-04\n",
       "accumul    8.085382e-05\n",
       "accus      2.425614e-04\n",
       "achiev     1.617076e-04\n",
       "acknowledg 8.085382e-05\n",
       "acorn      1.617076e-04\n",
       "acr        8.085382e-05\n",
       "across     6.468305e-04\n",
       "act        8.085382e-04\n",
       "action     4.042691e-04\n",
       "⋮          ⋮           \n",
       "window     1.617076e-04\n",
       "winner     1.617076e-04\n",
       "winter     8.085382e-05\n",
       "wipe       8.085382e-05\n",
       "wit        1.617076e-04\n",
       "withdraw   1.617076e-04\n",
       "within     6.468305e-04\n",
       "without    1.212807e-03\n",
       "woman      1.697930e-03\n",
       "wont       3.234153e-04\n",
       "word       8.085382e-04\n",
       "work       1.455369e-03\n",
       "worker     8.085382e-04\n",
       "workforc   2.425614e-04\n",
       "workman    1.617076e-04\n",
       "workplac   8.085382e-05\n",
       "world      1.697930e-03\n",
       "worri      8.085382e-05\n",
       "worth      7.276843e-04\n",
       "wound      8.085382e-05\n",
       "write      2.425614e-04\n",
       "wrong      2.425614e-04\n",
       "year       1.261320e-02\n",
       "yes        8.085382e-05\n",
       "yet        3.234153e-04\n",
       "young      6.468305e-04\n",
       "youth      1.617076e-04\n",
       "zero       8.893920e-04\n",
       "zip        1.617076e-04\n",
       "zone       1.617076e-04"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model$condprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the result from the training to test the accuracy of the produced model on the validation set. The accuracy is simply defined as the number of the correct predicted labels; for a more deep analysis we also provide the confusion matrix, in order to see if specific patterns are present (for example a label which is predicted much more times than the others without any reason). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pred_labels <- sapply(validation_set$Text, function(doc) {\n",
    "  apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1848958 \n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "True  0  1  2  3  4  5\n",
      "   0  9 25 24 26 33 16\n",
      "   1  7 61 45 79 81 36\n",
      "   2 14 46 37 57 46 37\n",
      "   3 10 51 58 90 53 44\n",
      "   4 22 71 48 79 60 36\n",
      "   5 14 46 36 66 46 27\n"
     ]
    }
   ],
   "source": [
    "correct_predictions <- sum(test_set$Label == pred_labels)\n",
    "total_predictions <- length(test_set$Label)\n",
    "accuracy <- correct_predictions / total_predictions\n",
    "confusion_matrix <- table(True = test_set$Label, Predicted = pred_labels)\n",
    "\n",
    "cat(\"Accuracy:\", accuracy, \"\\n\\n\")\n",
    "cat(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the accuracy obtained on the validation set is really low. Our model performs a little better than choosing at random (which will give an average accuracy of 0.167, 1 over 6), but obviously this result indicates that this methods is not capable of classifying well the documents. From the conclusion matrix we see that no specific pattern arises and in general we don't have a general behaviour that explains the misclassified documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of the hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only parameter that we can tune using the validation set in this case is the occurrency threshold for our vocabulary. In order to find the best parameter, we can simply train different models and choose the one that maximizes the accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 20 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>accuracy</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>0.2311198</td></tr>\n",
       "\t<tr><td> 2</td><td>0.2259115</td></tr>\n",
       "\t<tr><td> 3</td><td>0.2207031</td></tr>\n",
       "\t<tr><td> 4</td><td>0.2167969</td></tr>\n",
       "\t<tr><td> 5</td><td>0.2174479</td></tr>\n",
       "\t<tr><td> 6</td><td>0.2174479</td></tr>\n",
       "\t<tr><td> 7</td><td>0.2180990</td></tr>\n",
       "\t<tr><td> 8</td><td>0.2187500</td></tr>\n",
       "\t<tr><td> 9</td><td>0.2200521</td></tr>\n",
       "\t<tr><td>10</td><td>0.2213542</td></tr>\n",
       "\t<tr><td>11</td><td>0.2187500</td></tr>\n",
       "\t<tr><td>12</td><td>0.2154948</td></tr>\n",
       "\t<tr><td>13</td><td>0.2154948</td></tr>\n",
       "\t<tr><td>14</td><td>0.2207031</td></tr>\n",
       "\t<tr><td>15</td><td>0.2220052</td></tr>\n",
       "\t<tr><td>16</td><td>0.2213542</td></tr>\n",
       "\t<tr><td>17</td><td>0.2154948</td></tr>\n",
       "\t<tr><td>18</td><td>0.2135417</td></tr>\n",
       "\t<tr><td>19</td><td>0.2102865</td></tr>\n",
       "\t<tr><td>20</td><td>0.2044271</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 20 × 2\n",
       "\\begin{tabular}{ll}\n",
       " threshold & accuracy\\\\\n",
       " <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  1 & 0.2311198\\\\\n",
       "\t  2 & 0.2259115\\\\\n",
       "\t  3 & 0.2207031\\\\\n",
       "\t  4 & 0.2167969\\\\\n",
       "\t  5 & 0.2174479\\\\\n",
       "\t  6 & 0.2174479\\\\\n",
       "\t  7 & 0.2180990\\\\\n",
       "\t  8 & 0.2187500\\\\\n",
       "\t  9 & 0.2200521\\\\\n",
       "\t 10 & 0.2213542\\\\\n",
       "\t 11 & 0.2187500\\\\\n",
       "\t 12 & 0.2154948\\\\\n",
       "\t 13 & 0.2154948\\\\\n",
       "\t 14 & 0.2207031\\\\\n",
       "\t 15 & 0.2220052\\\\\n",
       "\t 16 & 0.2213542\\\\\n",
       "\t 17 & 0.2154948\\\\\n",
       "\t 18 & 0.2135417\\\\\n",
       "\t 19 & 0.2102865\\\\\n",
       "\t 20 & 0.2044271\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 20 × 2\n",
       "\n",
       "| threshold &lt;int&gt; | accuracy &lt;dbl&gt; |\n",
       "|---|---|\n",
       "|  1 | 0.2311198 |\n",
       "|  2 | 0.2259115 |\n",
       "|  3 | 0.2207031 |\n",
       "|  4 | 0.2167969 |\n",
       "|  5 | 0.2174479 |\n",
       "|  6 | 0.2174479 |\n",
       "|  7 | 0.2180990 |\n",
       "|  8 | 0.2187500 |\n",
       "|  9 | 0.2200521 |\n",
       "| 10 | 0.2213542 |\n",
       "| 11 | 0.2187500 |\n",
       "| 12 | 0.2154948 |\n",
       "| 13 | 0.2154948 |\n",
       "| 14 | 0.2207031 |\n",
       "| 15 | 0.2220052 |\n",
       "| 16 | 0.2213542 |\n",
       "| 17 | 0.2154948 |\n",
       "| 18 | 0.2135417 |\n",
       "| 19 | 0.2102865 |\n",
       "| 20 | 0.2044271 |\n",
       "\n"
      ],
      "text/plain": [
       "   threshold accuracy \n",
       "1   1        0.2311198\n",
       "2   2        0.2259115\n",
       "3   3        0.2207031\n",
       "4   4        0.2167969\n",
       "5   5        0.2174479\n",
       "6   6        0.2174479\n",
       "7   7        0.2180990\n",
       "8   8        0.2187500\n",
       "9   9        0.2200521\n",
       "10 10        0.2213542\n",
       "11 11        0.2187500\n",
       "12 12        0.2154948\n",
       "13 13        0.2154948\n",
       "14 14        0.2207031\n",
       "15 15        0.2220052\n",
       "16 16        0.2213542\n",
       "17 17        0.2154948\n",
       "18 18        0.2135417\n",
       "19 19        0.2102865\n",
       "20 20        0.2044271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poss_thresholds <- 1:20\n",
    "val_results <- validation(dataset, poss_thresholds, type = \"Six\")\n",
    "val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold:  1 \n",
      "Best accuracy:  0.2311198 \n"
     ]
    }
   ],
   "source": [
    "best_threshold <- val_results$threshold[which.max(val_results$accuracy)]\n",
    "cat(\"Best threshold: \", best_threshold, \"\\n\")\n",
    "cat(\"Best accuracy: \", max(val_results$accuracy), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we are able to tune the best threshold for our model: as we can see, even after a tuning, we still obtain a really small value for the accuracy, which indicates that this parameter is not the main responsable for the poor performances of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the choice of the bets hyper-parameters we proceed testing the model on unseen data, the test set. We train again the model with the best threshold for the vocabulary and then we study the accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model <- train_multinomial_nb(classes, training_set, best_threshold, type = \"Six\")\n",
    "\n",
    "pred_labels <- sapply(test_set$Text, function(doc) {\n",
    "  apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2395833 \n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "True   0   1   2   3   4   5\n",
      "   0   4  36  14  41  20  18\n",
      "   1   6  73  37  96  66  31\n",
      "   2   7  52  43  68  47  20\n",
      "   3   4  45  44 113  78  22\n",
      "   4   5  62  38  77 100  34\n",
      "   5   5  39  32  72  52  35\n"
     ]
    }
   ],
   "source": [
    "correct_predictions <- sum(test_set$Label == pred_labels)\n",
    "total_predictions <- length(test_set$Label)\n",
    "accuracy <- correct_predictions / total_predictions\n",
    "confusion_matrix <- table(True = test_set$Label, Predicted = pred_labels)\n",
    "\n",
    "cat(\"Accuracy:\", accuracy, \"\\n\\n\")\n",
    "cat(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this final analysis we obtain again a very low accuracy for our model; again no specific pattern can be deduced from the confusion matrix.\n",
    "\n",
    "One thing that in general we can conclude is that we don't have overfitting or underfitting as the training, the validation and the test errors are all similar. One possible cause of the poor performance is the small length of each document in the dataset, which makes hard for the model to classify only on the basis of a few words; at the same time, the presence of six different lables makes things more difficult for the model, as similar labels could share similar general patterns (this is amplified by the small number of words per document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible reason for the poor performance of the model is a not enough large dataset for training and validation; in order to remove this possibility we proceed using the K-fold cross validation approach. In the following cells, we perform the same operations done in the previous points, studying possible values for the threshold. Moreover, this time we divide the dataset only in training set and test set, as the validation set is directly selected by the `kfold_cross_validation` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "eigthy_percent <- floor(length(dataset$Text) * 0.8)\n",
    "n <- nrow(dataset)\n",
    "\n",
    "dataset <- dataset[sample(n), ]\n",
    "\n",
    "training_set <- dataset[1:eigthy_percent, ]\n",
    "test_set <- dataset[(eigthy_percent + 1):n, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 20 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>mean_accuracy</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>0.2260232</td></tr>\n",
       "\t<tr><td> 2</td><td>0.2266341</td></tr>\n",
       "\t<tr><td> 3</td><td>0.2267563</td></tr>\n",
       "\t<tr><td> 4</td><td>0.2222358</td></tr>\n",
       "\t<tr><td> 5</td><td>0.2223580</td></tr>\n",
       "\t<tr><td> 6</td><td>0.2217471</td></tr>\n",
       "\t<tr><td> 7</td><td>0.2216249</td></tr>\n",
       "\t<tr><td> 8</td><td>0.2200367</td></tr>\n",
       "\t<tr><td> 9</td><td>0.2197923</td></tr>\n",
       "\t<tr><td>10</td><td>0.2200367</td></tr>\n",
       "\t<tr><td>11</td><td>0.2221136</td></tr>\n",
       "\t<tr><td>12</td><td>0.2217471</td></tr>\n",
       "\t<tr><td>13</td><td>0.2184484</td></tr>\n",
       "\t<tr><td>14</td><td>0.2191814</td></tr>\n",
       "\t<tr><td>15</td><td>0.2191814</td></tr>\n",
       "\t<tr><td>16</td><td>0.2196701</td></tr>\n",
       "\t<tr><td>17</td><td>0.2202810</td></tr>\n",
       "\t<tr><td>18</td><td>0.2218693</td></tr>\n",
       "\t<tr><td>19</td><td>0.2211362</td></tr>\n",
       "\t<tr><td>20</td><td>0.2195480</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 20 × 2\n",
       "\\begin{tabular}{ll}\n",
       " threshold & mean\\_accuracy\\\\\n",
       " <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  1 & 0.2260232\\\\\n",
       "\t  2 & 0.2266341\\\\\n",
       "\t  3 & 0.2267563\\\\\n",
       "\t  4 & 0.2222358\\\\\n",
       "\t  5 & 0.2223580\\\\\n",
       "\t  6 & 0.2217471\\\\\n",
       "\t  7 & 0.2216249\\\\\n",
       "\t  8 & 0.2200367\\\\\n",
       "\t  9 & 0.2197923\\\\\n",
       "\t 10 & 0.2200367\\\\\n",
       "\t 11 & 0.2221136\\\\\n",
       "\t 12 & 0.2217471\\\\\n",
       "\t 13 & 0.2184484\\\\\n",
       "\t 14 & 0.2191814\\\\\n",
       "\t 15 & 0.2191814\\\\\n",
       "\t 16 & 0.2196701\\\\\n",
       "\t 17 & 0.2202810\\\\\n",
       "\t 18 & 0.2218693\\\\\n",
       "\t 19 & 0.2211362\\\\\n",
       "\t 20 & 0.2195480\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 20 × 2\n",
       "\n",
       "| threshold &lt;int&gt; | mean_accuracy &lt;dbl&gt; |\n",
       "|---|---|\n",
       "|  1 | 0.2260232 |\n",
       "|  2 | 0.2266341 |\n",
       "|  3 | 0.2267563 |\n",
       "|  4 | 0.2222358 |\n",
       "|  5 | 0.2223580 |\n",
       "|  6 | 0.2217471 |\n",
       "|  7 | 0.2216249 |\n",
       "|  8 | 0.2200367 |\n",
       "|  9 | 0.2197923 |\n",
       "| 10 | 0.2200367 |\n",
       "| 11 | 0.2221136 |\n",
       "| 12 | 0.2217471 |\n",
       "| 13 | 0.2184484 |\n",
       "| 14 | 0.2191814 |\n",
       "| 15 | 0.2191814 |\n",
       "| 16 | 0.2196701 |\n",
       "| 17 | 0.2202810 |\n",
       "| 18 | 0.2218693 |\n",
       "| 19 | 0.2211362 |\n",
       "| 20 | 0.2195480 |\n",
       "\n"
      ],
      "text/plain": [
       "   threshold mean_accuracy\n",
       "1   1        0.2260232    \n",
       "2   2        0.2266341    \n",
       "3   3        0.2267563    \n",
       "4   4        0.2222358    \n",
       "5   5        0.2223580    \n",
       "6   6        0.2217471    \n",
       "7   7        0.2216249    \n",
       "8   8        0.2200367    \n",
       "9   9        0.2197923    \n",
       "10 10        0.2200367    \n",
       "11 11        0.2221136    \n",
       "12 12        0.2217471    \n",
       "13 13        0.2184484    \n",
       "14 14        0.2191814    \n",
       "15 15        0.2191814    \n",
       "16 16        0.2196701    \n",
       "17 17        0.2202810    \n",
       "18 18        0.2218693    \n",
       "19 19        0.2211362    \n",
       "20 20        0.2195480    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poss_thresholds <- 1:20\n",
    "crossval_results <- kfold_cross_validation(training_set, k = 5, thresholds = poss_thresholds, type = \"Six\")\n",
    "crossval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "3"
      ],
      "text/latex": [
       "3"
      ],
      "text/markdown": [
       "3"
      ],
      "text/plain": [
       "[1] 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_threshold <- crossval_results$threshold[which.max(crossval_results$mean_accuracy)]\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model <- train_multinomial_nb(classes, training_set, best_threshold, type = \"Six\")\n",
    "pred_labels <- sapply(test_set$Text, function(doc) {\n",
    "  apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.237793 \n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "True   0   1   2   3   4   5\n",
      "   0  22  50  36  30  25  19\n",
      "   1  25 100  55  85  76  54\n",
      "   2  15  84  62  85  68  25\n",
      "   3  12  85  53 127  83  51\n",
      "   4  11  62  46  99 101  69\n",
      "   5  15  52  33  72  86  75\n"
     ]
    }
   ],
   "source": [
    "correct_predictions <- sum(test_set$Label == pred_labels)\n",
    "total_predictions <- length(test_set$Label)\n",
    "accuracy <- correct_predictions / total_predictions\n",
    "confusion_matrix <- table(True = test_set$Label, Predicted = pred_labels)\n",
    "\n",
    "cat(\"Accuracy:\", accuracy, \"\\n\\n\")\n",
    "cat(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this approach we obtain similar results as before. Depending on the initial random shuffling of the dataset we obtain values of accuracies for the best threshold between 0.20 and 0.23, which is still an indicator of a very bad performance of our model. In any case, this result tells us that the k-fold cross validation doesn't change a lot the behaviour of the model; this could indicate the necessity of a different pre-processing technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis using tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approaches used up to this point have not produced a succesfull model. As already anticipated, probably the low number of words for document is one of the biggest problems for the performance of our model: for this reason, we leverage the presence of the column `Tag`, building the vocabulary in a different way. Rather than looking to all the document, we consider the different tags and build a different vocabulary for each tag: then we unify the vocabularies in a single one. The idea behind this process is that for different tags we have different main words and more words are under the threshold (and thus not considered). We load again the dataset in order to prove that the vocabulary obtained this way is smaller than with the previous approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset <- read.csv(\"six_label_dataset.csv\", col.names = c(\"Label\", \"Text\", \"Tag\"))\n",
    "dataset$Label <- change_labels(dataset$Label)\n",
    "classes <- as.integer(sort(unique(dataset$Label)))\n",
    "args <- sort(unique(unlist(strsplit(dataset$Tag, \",\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "21662"
      ],
      "text/latex": [
       "21662"
      ],
      "text/markdown": [
       "21662"
      ],
      "text/plain": [
       "[1] 21662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc <- length(get_vocabulary_tags(dataset, threshold = 1))\n",
    "len_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset$Text <- clean(dataset$Text)\n",
    "dataset <- clean_empty_rows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1149"
      ],
      "text/latex": [
       "1149"
      ],
      "text/markdown": [
       "1149"
      ],
      "text/plain": [
       "[1] 1149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc <- length(get_vocabulary_tags(dataset, threshold = 5))\n",
    "len_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using `threshold = 5` in this case we able to reduce the vocabulary to 5.3% of the initial vocabulary. Next, we proceed to a k-fold cross validation in order to select the best threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 21 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>mean_accuracy</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0</td><td>0.2259010</td></tr>\n",
       "\t<tr><td> 1</td><td>0.2259010</td></tr>\n",
       "\t<tr><td> 2</td><td>0.2254123</td></tr>\n",
       "\t<tr><td> 3</td><td>0.2245571</td></tr>\n",
       "\t<tr><td> 4</td><td>0.2250458</td></tr>\n",
       "\t<tr><td> 5</td><td>0.2218693</td></tr>\n",
       "\t<tr><td> 6</td><td>0.2188149</td></tr>\n",
       "\t<tr><td> 7</td><td>0.2196701</td></tr>\n",
       "\t<tr><td> 8</td><td>0.2216249</td></tr>\n",
       "\t<tr><td> 9</td><td>0.2186927</td></tr>\n",
       "\t<tr><td>10</td><td>0.2200367</td></tr>\n",
       "\t<tr><td>11</td><td>0.2208919</td></tr>\n",
       "\t<tr><td>12</td><td>0.2241906</td></tr>\n",
       "\t<tr><td>13</td><td>0.2210141</td></tr>\n",
       "\t<tr><td>14</td><td>0.2182040</td></tr>\n",
       "\t<tr><td>15</td><td>0.2186927</td></tr>\n",
       "\t<tr><td>16</td><td>0.2197923</td></tr>\n",
       "\t<tr><td>17</td><td>0.2202810</td></tr>\n",
       "\t<tr><td>18</td><td>0.2178375</td></tr>\n",
       "\t<tr><td>19</td><td>0.2205254</td></tr>\n",
       "\t<tr><td>20</td><td>0.2174710</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 21 × 2\n",
       "\\begin{tabular}{ll}\n",
       " threshold & mean\\_accuracy\\\\\n",
       " <int> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  0 & 0.2259010\\\\\n",
       "\t  1 & 0.2259010\\\\\n",
       "\t  2 & 0.2254123\\\\\n",
       "\t  3 & 0.2245571\\\\\n",
       "\t  4 & 0.2250458\\\\\n",
       "\t  5 & 0.2218693\\\\\n",
       "\t  6 & 0.2188149\\\\\n",
       "\t  7 & 0.2196701\\\\\n",
       "\t  8 & 0.2216249\\\\\n",
       "\t  9 & 0.2186927\\\\\n",
       "\t 10 & 0.2200367\\\\\n",
       "\t 11 & 0.2208919\\\\\n",
       "\t 12 & 0.2241906\\\\\n",
       "\t 13 & 0.2210141\\\\\n",
       "\t 14 & 0.2182040\\\\\n",
       "\t 15 & 0.2186927\\\\\n",
       "\t 16 & 0.2197923\\\\\n",
       "\t 17 & 0.2202810\\\\\n",
       "\t 18 & 0.2178375\\\\\n",
       "\t 19 & 0.2205254\\\\\n",
       "\t 20 & 0.2174710\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 21 × 2\n",
       "\n",
       "| threshold &lt;int&gt; | mean_accuracy &lt;dbl&gt; |\n",
       "|---|---|\n",
       "|  0 | 0.2259010 |\n",
       "|  1 | 0.2259010 |\n",
       "|  2 | 0.2254123 |\n",
       "|  3 | 0.2245571 |\n",
       "|  4 | 0.2250458 |\n",
       "|  5 | 0.2218693 |\n",
       "|  6 | 0.2188149 |\n",
       "|  7 | 0.2196701 |\n",
       "|  8 | 0.2216249 |\n",
       "|  9 | 0.2186927 |\n",
       "| 10 | 0.2200367 |\n",
       "| 11 | 0.2208919 |\n",
       "| 12 | 0.2241906 |\n",
       "| 13 | 0.2210141 |\n",
       "| 14 | 0.2182040 |\n",
       "| 15 | 0.2186927 |\n",
       "| 16 | 0.2197923 |\n",
       "| 17 | 0.2202810 |\n",
       "| 18 | 0.2178375 |\n",
       "| 19 | 0.2205254 |\n",
       "| 20 | 0.2174710 |\n",
       "\n"
      ],
      "text/plain": [
       "   threshold mean_accuracy\n",
       "1   0        0.2259010    \n",
       "2   1        0.2259010    \n",
       "3   2        0.2254123    \n",
       "4   3        0.2245571    \n",
       "5   4        0.2250458    \n",
       "6   5        0.2218693    \n",
       "7   6        0.2188149    \n",
       "8   7        0.2196701    \n",
       "9   8        0.2216249    \n",
       "10  9        0.2186927    \n",
       "11 10        0.2200367    \n",
       "12 11        0.2208919    \n",
       "13 12        0.2241906    \n",
       "14 13        0.2210141    \n",
       "15 14        0.2182040    \n",
       "16 15        0.2186927    \n",
       "17 16        0.2197923    \n",
       "18 17        0.2202810    \n",
       "19 18        0.2178375    \n",
       "20 19        0.2205254    \n",
       "21 20        0.2174710    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poss_thresholds <- 0:20\n",
    "crossval_results <- kfold_cross_validation(training_set, k = 5, thresholds = poss_thresholds, type = \"Tags\")\n",
    "crossval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_threshold <- crossval_results$threshold[which.max(crossval_results$mean_accuracy)]\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model <- train_multinomial_nb(classes, training_set, best_threshold, type = \"Tags\")\n",
    "pred_labels <- sapply(test_set$Text, function(doc) {\n",
    "  apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2421875 \n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "True   0   1   2   3   4   5\n",
      "   0  16  47  34  42  26  17\n",
      "   1  15  98  52 103  80  47\n",
      "   2   8  83  62  89  71  26\n",
      "   3   6  80  51 146  81  47\n",
      "   4   6  60  38 120 106  58\n",
      "   5   6  55  31  81  92  68\n"
     ]
    }
   ],
   "source": [
    "correct_predictions <- sum(test_set$Label == pred_labels)\n",
    "total_predictions <- length(test_set$Label)\n",
    "accuracy <- correct_predictions / total_predictions\n",
    "confusion_matrix <- table(True = test_set$Label, Predicted = pred_labels)\n",
    "\n",
    "cat(\"Accuracy:\", accuracy, \"\\n\\n\")\n",
    "cat(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, also in this case, we are not able to achieve an accuracy higher than 25%, thus we can conclude that also this approach is not correct. The only thing that we can observe is that reducing the size of the vocabulary without any other kind of preprocessing doesn't really produce any gain in the accuracy; thus, this is probably not the best strategy for this dataset and other possibilities should be studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Two-label dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset is composed of documents labelled either 0 (reliable) or 1 (unreliable); due to the large number of document and also to the large number of words per document, we don't print lot of results in order to leave the notebook lighter. This time when computing the vocabulary, we use a frequency threshold instead of an occurrency threshold, as the number of words for document is much higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset <- read.csv(\"two_label_dataset.csv\", col.names = c(\"ID\", \"Title\", \"Author\", \"Text\", \"Label\"))\n",
    "classes <- as.integer(sort(unique(dataset$Label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "491555"
      ],
      "text/latex": [
       "491555"
      ],
      "text/markdown": [
       "491555"
      ],
      "text/plain": [
       "[1] 491555"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc <- length(get_vocabulary_two(dataset$Text, threshold = 0))\n",
    "len_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset$Text <- clean(dataset$Text)\n",
    "dataset <- clean_empty_rows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "25022"
      ],
      "text/latex": [
       "25022"
      ],
      "text/markdown": [
       "25022"
      ],
      "text/plain": [
       "[1] 25022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc <- length(get_vocabulary_two(dataset$Text, threshold = 0))\n",
    "len_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2758"
      ],
      "text/latex": [
       "2758"
      ],
      "text/markdown": [
       "2758"
      ],
      "text/plain": [
       "[1] 2758"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_voc <- length(get_vocabulary_two(dataset$Text, threshold = 5e-5))\n",
    "len_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case we can see that the cleaning process reduces a lot the total number of words that are actually unique in our dataset; in particular we get that, using the previously presented techniques for stemming and lemmatizing, the final vocabulary is only 5.1% of the initial vocabulary. If we include also a frequency check, choosing a threshold greater than 0, we are able to reduce the dimension of the vocabulary even more; for example, for `threshold = 5e-5`, the final vocabulary is only 0.5% of the initial vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous analysis we obtained that k-fold cross validation yields similar values of accuracy as normal validation; given the recognized strength of this method, we use it directly when analyzing the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "eighty_percent <- as.integer(length(dataset$Text) * 0.8)\n",
    "\n",
    "training_set <- dataset[1:eighty_percent, ]\n",
    "test_set <- dataset[(eighty_percent + 1):length(dataset$Text), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 9 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>threshold</th><th scope=col>mean_accuracy</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.0e-08</td><td>0.8670313</td></tr>\n",
       "\t<tr><td>1.0e-07</td><td>0.8670313</td></tr>\n",
       "\t<tr><td>5.0e-07</td><td>0.8669705</td></tr>\n",
       "\t<tr><td>1.0e-06</td><td>0.8659982</td></tr>\n",
       "\t<tr><td>5.0e-06</td><td>0.8635673</td></tr>\n",
       "\t<tr><td>1.0e-05</td><td>0.8615618</td></tr>\n",
       "\t<tr><td>1.6e-05</td><td>0.8586448</td></tr>\n",
       "\t<tr><td>2.0e-05</td><td>0.8576724</td></tr>\n",
       "\t<tr><td>5.0e-05</td><td>0.8497113</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 9 × 2\n",
       "\\begin{tabular}{ll}\n",
       " threshold & mean\\_accuracy\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 5.0e-08 & 0.8670313\\\\\n",
       "\t 1.0e-07 & 0.8670313\\\\\n",
       "\t 5.0e-07 & 0.8669705\\\\\n",
       "\t 1.0e-06 & 0.8659982\\\\\n",
       "\t 5.0e-06 & 0.8635673\\\\\n",
       "\t 1.0e-05 & 0.8615618\\\\\n",
       "\t 1.6e-05 & 0.8586448\\\\\n",
       "\t 2.0e-05 & 0.8576724\\\\\n",
       "\t 5.0e-05 & 0.8497113\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 9 × 2\n",
       "\n",
       "| threshold &lt;dbl&gt; | mean_accuracy &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 5.0e-08 | 0.8670313 |\n",
       "| 1.0e-07 | 0.8670313 |\n",
       "| 5.0e-07 | 0.8669705 |\n",
       "| 1.0e-06 | 0.8659982 |\n",
       "| 5.0e-06 | 0.8635673 |\n",
       "| 1.0e-05 | 0.8615618 |\n",
       "| 1.6e-05 | 0.8586448 |\n",
       "| 2.0e-05 | 0.8576724 |\n",
       "| 5.0e-05 | 0.8497113 |\n",
       "\n"
      ],
      "text/plain": [
       "  threshold mean_accuracy\n",
       "1 5.0e-08   0.8670313    \n",
       "2 1.0e-07   0.8670313    \n",
       "3 5.0e-07   0.8669705    \n",
       "4 1.0e-06   0.8659982    \n",
       "5 5.0e-06   0.8635673    \n",
       "6 1.0e-05   0.8615618    \n",
       "7 1.6e-05   0.8586448    \n",
       "8 2.0e-05   0.8576724    \n",
       "9 5.0e-05   0.8497113    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poss_thresholds <- c(5e-08, 1e-07, 5e-07, 1e-06, 5e-06, 1e-05, 1.6e-05, 2e-05, 5e-05)\n",
    "crossval_results <- kfold_cross_validation(training_set, k = 5, thresholds = poss_thresholds, type = \"Two\")\n",
    "crossval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "5e-08"
      ],
      "text/latex": [
       "5e-08"
      ],
      "text/markdown": [
       "5e-08"
      ],
      "text/plain": [
       "[1] 5e-08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_threshold <- crossval_results$threshold[which.max(crossval_results$mean_accuracy)]\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in this case we are able to obtain a much higher validation accuracy, probably due to both the presence of only two labels and of much longer documents, which help the model choosing the correct labelling. In any case, the best thresholds is chosen to be 5e-08, with a validation accuracy of 0.867. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model <- train_multinomial_nb(classes, training_set, best_threshold, type = \"Two\")\n",
    "pred_labels <- sapply(test_set$Text, function(doc) {\n",
    "  apply_multinomial_nb(classes, model$vocab, model$prior, model$condprob, doc)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8746051 \n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "True    0    1\n",
      "   0 1803  239\n",
      "   1  277 1796\n"
     ]
    }
   ],
   "source": [
    "correct_predictions <- sum(test_set$Label == pred_labels)\n",
    "total_predictions <- length(test_set$Label)\n",
    "accuracy <- correct_predictions / total_predictions\n",
    "confusion_matrix <- table(True = test_set$Label, Predicted = pred_labels)\n",
    "\n",
    "cat(\"Accuracy:\", accuracy, \"\\n\\n\")\n",
    "cat(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the testing we are able to gain an even higher accuracy, which indicates that the general strategy, when applied to a large enough dataset, works pretty well (even if some improvements can be still obtained). In particular for this dataset we should try to reduce the number of false-negative, as a fake-news classified reliable is much worse than the opposite; some specific techniques should be developed to cope with this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
